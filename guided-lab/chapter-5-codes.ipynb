{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyP4rnZkdA1AyLlrvpXEz8z/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 5.1 Parts of Speech (POS) Tagging"],"metadata":{"id":"NnVA8vCBfSnB"}},{"cell_type":"markdown","source":["5.1.2 Implementing POS Tagging in Python"],"metadata":{"id":"m85IPdDZfXdY"}},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"id":"mmCze9OSfZKG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk import word_tokenize, pos_tag\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","# Sample text\n","text = \"Natural Language Processing with Python is fascinating.\"\n","\n","# Tokenize the text into words\n","tokens = word_tokenize(text)\n","\n","# Perform POS tagging\n","pos_tags = pos_tag(tokens)\n","\n","print(\"POS Tags:\")\n","print(pos_tags)"],"metadata":{"id":"xHkKGrSdfae3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5.1.3 Evaluating POS Taggers"],"metadata":{"id":"_CECV7g2fcYe"}},{"cell_type":"code","source":["from nltk import pos_tag\n","from nltk.corpus import treebank\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import nltk\n","nltk.download('treebank')\n","\n","# Load the treebank corpus\n","test_data = treebank.tagged_sents()[3000:]\n","test_sentences = [[word for word, tag in sent] for sent in test_data]\n","gold_standard = [[tag for word, tag in sent] for sent in test_data]\n","\n","# Tag the test sentences using a pre-trained tagger\n","tagger = nltk.PerceptronTagger()\n","predicted_tags = [tagger.tag(sent) for sent in test_sentences]\n","predicted_tags = [[tag for word, tag in sent] for sent in predicted_tags]\n","\n","# Flatten the lists to compute metrics\n","gold_standard_flat = [tag for sent in gold_standard for tag in sent]\n","predicted_tags_flat = [tag for sent in predicted_tags for tag in sent]\n","\n","# Compute evaluation metrics\n","accuracy = accuracy_score(gold_standard_flat, predicted_tags_flat)\n","precision = precision_score(gold_standard_flat, predicted_tags_flat, average='weighted')\n","recall = recall_score(gold_standard_flat, predicted_tags_flat, average='weighted')\n","f1 = f1_score(gold_standard_flat, predicted_tags_flat, average='weighted')\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1 Score:\", f1)"],"metadata":{"id":"4U4zT8Z0feEI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5.1.4 Training Custom POS Taggers"],"metadata":{"id":"chJ61ebFff6F"}},{"cell_type":"code","source":["from nltk.tag import UnigramTagger, BigramTagger\n","from nltk.corpus import treebank\n","nltk.download('treebank')\n","\n","# Load the treebank corpus\n","train_data = treebank.tagged_sents()[:3000]\n","test_data = treebank.tagged_sents()[3000:]\n","\n","# Train a UnigramTagger\n","unigram_tagger = UnigramTagger(train_data)\n","\n","# Evaluate the tagger\n","accuracy = unigram_tagger.evaluate(test_data)\n","print(\"Unigram Tagger Accuracy:\", accuracy)\n","\n","# Train a BigramTagger backed by the UnigramTagger\n","bigram_tagger = BigramTagger(train_data, backoff=unigram_tagger)\n","\n","# Evaluate the tagger\n","accuracy = bigram_tagger.evaluate(test_data)\n","print(\"Bigram Tagger Accuracy:\", accuracy)"],"metadata":{"id":"Zb7c3zYufknk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5.2 Named Entity Recognition (NER)"],"metadata":{"id":"Xy_S5g5mfpCl"}},{"cell_type":"markdown","source":["5.2.2 Implementing NER in Python"],"metadata":{"id":"oM2XCHIYfrKT"}},{"cell_type":"code","source":["!pip install spacy\n","!python -m spacy download en_core_web_sm"],"metadata":{"id":"FLdcKZz4fs0c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","\n","# Load the pre-trained spaCy model\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Sample text\n","text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n","\n","# Process the text with the spaCy model\n","doc = nlp(text)\n","\n","# Print named entities with their labels\n","print(\"Named Entities:\")\n","for ent in doc.ents:\n","    print(ent.text, ent.label_)"],"metadata":{"id":"FQMMKsV7f0_8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5.2.3 Evaluating NER Systems"],"metadata":{"id":"c4ieGXK-gDVx"}},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","# True entities in the text (manually annotated)\n","true_entities = [\"Apple\", \"U.K.\", \"startup\", \"$1 billion\"]\n","\n","# Entities identified by the NER system\n","predicted_entities = [\"Apple\", \"UK\", \"startup\", \"$1B\"]\n","\n","# Calculate precision, recall, and F1 score\n","precision = precision_score(true_entities, predicted_entities, average='micro')\n","recall = recall_score(true_entities, predicted_entities, average='micro')\n","f1 = f1_score(true_entities, predicted_entities, average='micro')\n","\n","print(f\"Precision: {precision}\")\n","print(f\"Recall: {recall}\")\n","print(f\"F1 Score: {f1}\")"],"metadata":{"id":"a689sgYRgFv9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5.2.4 Training Custom NER Models"],"metadata":{"id":"5pfISUisgInf"}},{"cell_type":"code","source":["import spacy\n","from spacy.tokens import DocBin\n","from spacy.training import Example\n","from spacy.util import minibatch, compounding\n","\n","# Create a blank English model\n","nlp = spacy.blank(\"en\")\n","\n","# Create a new NER component and add it to the pipeline\n","ner = nlp.add_pipe(\"ner\")\n","\n","# Add labels to the NER component\n","ner.add_label(\"GADGET\")\n","\n","# Sample training data\n","TRAIN_DATA = [\n","    (\"Apple is releasing a new iPhone.\", {\"entities\": [(26, 32, \"GADGET\")]}),\n","    (\"The new iPad Pro is amazing.\", {\"entities\": [(8, 16, \"GADGET\")]}),\n","]\n","\n","# Convert the training data to spaCy's format\n","doc_bin = DocBin()\n","for text, annotations in TRAIN_DATA:\n","    doc = nlp.make_doc(text)\n","    example = Example.from_dict(doc, annotations)\n","    doc_bin.add(example.reference)\n","\n","# Load the training data\n","examples = doc_bin.get_docs(nlp.vocab)\n","\n","# Train the NER model\n","optimizer = nlp.begin_training()\n","for epoch in range(10):\n","    losses = {}\n","    batches = minibatch(examples, size=compounding(4.0, 32.0, 1.001))\n","    for batch in batches:\n","        nlp.update(batch, drop=0.5, losses=losses)\n","    print(\"Losses\", losses)\n","\n","# Test the trained model\n","doc = nlp(\"I just bought a new iPhone.\")\n","print(\"Named Entities:\", [(ent.text, ent.label_) for ent in doc.ents])"],"metadata":{"id":"VoygGnCHgJI3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5.3 Dependency Parsing"],"metadata":{"id":"AcEioQgZgO1V"}},{"cell_type":"markdown","source":["5.3.2 Dependency Parsing with spaCy"],"metadata":{"id":"fUwHtgBygSDn"}},{"cell_type":"code","source":["import spacy\n","\n","# Load the pre-trained spaCy model\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Sample text\n","text = \"The cat sat on the mat.\"\n","\n","# Process the text with the spaCy model\n","doc = nlp(text)\n","\n","# Print dependency parsing results\n","print(\"Dependency Parsing:\")\n","for token in doc:\n","    print(f\"{token.text} ({token.dep_}): {token.head.text}\")\n","\n","# Visualize the dependency tree (requires Jupyter Notebook or similar environment)\n","from spacy import displacy\n","displacy.render(doc, style=\"dep\", jupyter=True)"],"metadata":{"id":"IW8NB659gVzT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5.3.4 Training Custom Dependency Parsers"],"metadata":{"id":"m4W7iaNhgY9E"}},{"cell_type":"code","source":["import spacy\n","from spacy.tokens import DocBin\n","from spacy.training import Example\n","from spacy.util import minibatch, compounding\n","\n","# Create a blank English model\n","nlp = spacy.blank(\"en\")\n","\n","# Create a new parser component and add it to the pipeline\n","parser = nlp.add_pipe(\"parser\")\n","\n","# Define labels for the parser\n","parser.add_label(\"nsubj\")\n","parser.add_label(\"dobj\")\n","parser.add_label(\"prep\")\n","\n","# Sample training data\n","TRAIN_DATA = [\n","    (\"She enjoys playing tennis.\", {\"heads\": [1, 1, 1, 2, 1], \"deps\": [\"nsubj\", \"ROOT\", \"aux\", \"prep\", \"pobj\"]}),\n","    (\"I like reading books.\", {\"heads\": [1, 1, 2, 1], \"deps\": [\"nsubj\", \"ROOT\", \"dobj\", \"punct\"]}),\n","]\n","\n","# Convert the training data to spaCy's format\n","doc_bin = DocBin()\n","for text, annotations in TRAIN_DATA:\n","    doc = nlp.make_doc(text)\n","    example = Example.from_dict(doc, annotations)\n","    doc_bin.add(example.reference)\n","\n","# Load the training data\n","examples = doc_bin.get_docs(nlp.vocab)\n","\n","# Train the parser\n","optimizer = nlp.begin_training()\n","for epoch in range(10):\n","    losses = {}\n","    batches = minibatch(examples, size=compounding(4.0, 32.0, 1.001))\n","    for batch in batches:\n","        nlp.update(batch, drop=0.5, losses=losses)\n","    print(\"Losses\", losses)\n","\n","# Test the trained model\n","doc = nlp(\"She enjoys reading books.\")\n","for token in doc:\n","    print(f\"{token.text} ({token.dep_}): {token.head.text}\")"],"metadata":{"id":"isW2lqVCgZav"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Chapter-5 Assignments"],"metadata":{"id":"xzBKvkZpgcYG"}},{"cell_type":"markdown","source":["Exercise 1: Parts of Speech (POS) Tagging"],"metadata":{"id":"bWNfsFe2gmo3"}},{"cell_type":"code","source":["import nltk\n","from nltk import word_tokenize, pos_tag\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","# Sample text\n","text = \"The quick brown fox jumps over the lazy dog.\"\n","\n","# Tokenize the text into words\n","tokens = word_tokenize(text)\n","\n","# Perform POS tagging\n","pos_tags = pos_tag(tokens)\n","\n","print(\"POS Tags:\")\n","print(pos_tags)"],"metadata":{"id":"5RWaVIIMgpRG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"_tMV-wfxhIG2"}},{"cell_type":"markdown","source":["Exercise 2: Named Entity Recognition (NER)"],"metadata":{"id":"pCNUbGU6gq1m"}},{"cell_type":"code","source":["import spacy\n","\n","# Load the pre-trained spaCy model\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Sample text\n","text = \"Barack Obama was born on August 4, 1961, in Honolulu, Hawaii.\"\n","\n","# Process the text with the spaCy model\n","doc = nlp(text)\n","\n","# Print named entities with their labels\n","print(\"Named Entities:\")\n","for ent in doc.ents:\n","    print(ent.text, ent.label_)"],"metadata":{"id":"AUpTLZuugvh6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"BLNHn1p0hI57"}},{"cell_type":"markdown","source":["Exercise 3: Training a Custom NER Model"],"metadata":{"id":"f2KROyP9gxwy"}},{"cell_type":"code","source":["import spacy\n","from spacy.tokens import DocBin\n","from spacy.training import Example\n","from spacy.util import minibatch, compounding\n","\n","# Create a blank English model\n","nlp = spacy.blank(\"en\")\n","\n","# Create a new NER component and add it to the pipeline\n","ner = nlp.add_pipe(\"ner\")\n","\n","# Add labels to the NER component\n","ner.add_label(\"GADGET\")\n","\n","# Sample training data\n","TRAIN_DATA = [\n","    (\"Apple is releasing a new iPhone.\", {\"entities\": [(26, 32, \"GADGET\")]}),\n","    (\"The new iPad Pro is amazing.\", {\"entities\": [(8, 16, \"GADGET\")]}),\n","]\n","\n","# Convert the training data to spaCy's format\n","doc_bin = DocBin()\n","for text, annotations in TRAIN_DATA:\n","    doc = nlp.make_doc(text)\n","    example = Example.from_dict(doc, annotations)\n","    doc_bin.add(example.reference)\n","\n","# Load the training data\n","examples = doc_bin.get_docs(nlp.vocab)\n","\n","# Train the NER model\n","optimizer = nlp.begin_training()\n","for epoch in range(10):\n","    losses = {}\n","    batches = minibatch(examples, size=compounding(4.0, 32.0, 1.001))\n","    for batch in batches:\n","        nlp.update(batch, drop=0.5, losses=losses)\n","    print(\"Losses\", losses)\n","\n","# Test the trained model\n","doc = nlp(\"I just bought a new iPhone.\")\n","print(\"Named Entities:\", [(ent.text, ent.label_) for ent in doc.ents])"],"metadata":{"id":"O7rVcUlPgzZn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"i7tKi5y_hJuA"}},{"cell_type":"markdown","source":["Exercise 4: Dependency Parsing"],"metadata":{"id":"KhRl3sZyg24X"}},{"cell_type":"code","source":["import spacy\n","\n","# Load the pre-trained spaCy model\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Sample text\n","text = \"She enjoys reading books.\"\n","\n","# Process the text with the spaCy model\n","doc = nlp(text)\n","\n","# Print dependency parsing results\n","print(\"Dependency Parsing:\")\n","for token in doc:\n","    print(f\"{token.text} ({token.dep_}): {token.head.text}\")\n","\n","# Visualize the dependency tree (requires jupyter notebook or similar environment)\n","from spacy import displacy\n","displacy.render(doc, style=\"dep\", jupyter=True)"],"metadata":{"id":"js026ygzg3h5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"InnsROwKhKlZ"}},{"cell_type":"markdown","source":["Exercise 5: Training a Custom Dependency Parser"],"metadata":{"id":"TCkXCHt1g6SL"}},{"cell_type":"code","source":["from spacy.tokens import DocBin\n","from spacy.training import Example\n","from spacy.util import minibatch, compounding\n","\n","# Create a blank English model\n","nlp = spacy.blank(\"en\")\n","\n","# Create a new parser component and add it to the pipeline\n","parser = nlp.add_pipe(\"parser\")\n","\n","# Define labels for the parser\n","parser.add_label(\"nsubj\")\n","parser.add_label(\"dobj\")\n","parser.add_label(\"prep\")\n","\n","# Sample training data\n","TRAIN_DATA = [\n","    (\"She enjoys playing tennis.\", {\"heads\": [1, 1, 1, 2, 1], \"deps\": [\"nsubj\", \"ROOT\", \"aux\", \"prep\", \"pobj\"]}),\n","    (\"I like reading books.\", {\"heads\": [1, 1, 2, 1], \"deps\": [\"nsubj\", \"ROOT\", \"dobj\", \"punct\"]}),\n","]\n","\n","# Convert the training data to spaCy's format\n","doc_bin = DocBin()\n","for text, annotations in TRAIN_DATA:\n","    doc = nlp.make_doc(text)\n","    example = Example.from_dict(doc, annotations)\n","    doc_bin.add(example.reference)\n","\n","# Load the training data\n","examples = doc_bin.get_docs(nlp.vocab)\n","\n","# Train the parser\n","optimizer = nlp.begin_training()\n","for epoch in range(10):\n","    losses = {}\n","    batches = minibatch(examples, size=compounding(4.0, 32.0, 1.001))\n","    for batch in batches:\n","        nlp.update(batch, drop=0.5, losses=losses)\n","    print(\"Losses\", losses)\n","\n","# Test the trained model\n","doc = nlp(\"She enjoys reading books.\")\n","for token in doc:\n","    print(f\"{token.text} ({token.dep_}): {token.head.text}\")"],"metadata":{"id":"F9Ttl18Cg6xA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"Xi1vDQe6hLWh"}}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMTv5APm7TV1iK7psPxHb/e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 3.1 Bag of Words"],"metadata":{"id":"OmCE-73MX4kO"}},{"cell_type":"markdown","source":["3.1.2 Implementing Bag of Words in Python\n"],"metadata":{"id":"TDGSXhuIX9D0"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Sample text corpus\n","documents = [\n","    \"Natural language processing is fun\",\n","    \"Language models are important in NLP\"\n","]\n","\n","# Initialize the CountVectorizer\n","vectorizer = CountVectorizer()\n","\n","# Fit the vectorizer on the text data\n","X = vectorizer.fit_transform(documents)\n","\n","# Convert the result to an array\n","bow_array = X.toarray()\n","\n","# Get the feature names (vocabulary)\n","vocab = vectorizer.get_feature_names_out()\n","\n","print(\"Vocabulary:\")\n","print(vocab)\n","\n","print(\"\\\\nBag of Words Array:\")\n","print(bow_array)"],"metadata":{"id":"FL9-wwNVX-sd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3.1.4 Practical Example: Text Classification with Bag of Words"],"metadata":{"id":"wecGpO_pYSxL"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Sample text corpus and labels\n","documents = [\n","    \"Natural language processing is fun\",\n","    \"Language models are important in NLP\",\n","    \"I enjoy learning about artificial intelligence\",\n","    \"Machine learning and NLP are closely related\",\n","    \"Deep learning is a subset of machine learning\"\n","]\n","labels = [1, 1, 0, 1, 0]  # 1 for NLP-related, 0 for AI-related\n","\n","# Initialize the CountVectorizer\n","vectorizer = CountVectorizer()\n","\n","# Transform the text data\n","X = vectorizer.fit_transform(documents)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n","\n","# Initialize the classifier\n","classifier = MultinomialNB()\n","\n","# Train the classifier\n","classifier.fit(X_train, y_train)\n","\n","# Predict the labels for the test set\n","y_pred = classifier.predict(X_test)\n","\n","# Calculate the accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)"],"metadata":{"id":"2OWFjegUYTkC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3.2 TF-IDFF"],"metadata":{"id":"QhgFwI_SYXBF"}},{"cell_type":"markdown","source":["3.2.3 Implementing TF-IDF in Python"],"metadata":{"id":"C6nDvtUxYnZ1"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Sample text corpus\n","documents = [\n","    \"Natural language processing is fun\",\n","    \"Language models are important in NLP\",\n","    \"I enjoy learning about artificial intelligence\",\n","    \"Machine learning and NLP are closely related\",\n","    \"Deep learning is a subset of machine learning\"\n","]\n","\n","# Initialize the TfidfVectorizer\n","vectorizer = TfidfVectorizer()\n","\n","# Fit the vectorizer on the text data\n","X = vectorizer.fit_transform(documents)\n","\n","# Convert the result to an array\n","tfidf_array = X.toarray()\n","\n","# Get the feature names (vocabulary)\n","vocab = vectorizer.get_feature_names_out()\n","\n","print(\"Vocabulary:\")\n","print(vocab)\n","\n","print(\"\\\\nTF-IDF Array:\")\n","print(tfidf_array)"],"metadata":{"id":"D-gx1ByyYoZh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3.2.4 Practical Example: Text Classification with TF-IDF"],"metadata":{"id":"3jwWDILtYshX"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Sample text corpus and labels\n","documents = [\n","    \"Natural language processing is fun\",\n","    \"Language models are important in NLP\",\n","    \"I enjoy learning about artificial intelligence\",\n","    \"Machine learning and NLP are closely related\",\n","    \"Deep learning is a subset of machine learning\"\n","]\n","labels = [1, 1, 0, 1, 0]  # 1 for NLP-related, 0 for AI-related\n","\n","# Initialize the TfidfVectorizer\n","vectorizer = TfidfVectorizer()\n","\n","# Transform the text data\n","X = vectorizer.fit_transform(documents)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n","\n","# Initialize the classifier\n","classifier = MultinomialNB()\n","\n","# Train the classifier\n","classifier.fit(X_train, y_train)\n","\n","# Predict the labels for the test set\n","y_pred = classifier.predict(X_test)\n","\n","# Calculate the accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)"],"metadata":{"id":"hsJG1bMaYtt0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3.3 Word Embeddings (Word2Vec, GloVe)"],"metadata":{"id":"8Q95Dwd_Y1p5"}},{"cell_type":"markdown","source":["3.3.2 Word2Vec"],"metadata":{"id":"XRga1RMzY47R"}},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"id":"3tJDYVDiZOBe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","import nltk\n","nltk.download('punkt')\n","\n","# Sample text corpus\n","text = \"Natural language processing is fun and exciting. Language models are important in NLP. I enjoy learning about artificial intelligence. Machine learning and NLP are closely related. Deep learning is a subset of machine learning.\"\n","\n","# Tokenize the text into sentences\n","sentences = sent_tokenize(text)\n","\n","# Tokenize each sentence into words\n","tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n","\n","# Train a Word2Vec model using the Skip-Gram method\n","model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, sg=1, min_count=1)\n","\n","# Get the vector representation of the word \"language\"\n","vector = model.wv['language']\n","print(\"Vector representation of 'language':\")\n","print(vector)\n","\n","# Find the most similar words to \"language\"\n","similar_words = model.wv.most_similar('language')\n","print(\"\\\\nMost similar words to 'language':\")\n","print(similar_words)"],"metadata":{"id":"jChlvR0SZxxP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3.3.3 GloVe (Global Vectors for Word Representation)"],"metadata":{"id":"2RgYahg1ZAXf"}},{"cell_type":"code","source":["import gensim.downloader as api\n","\n","# Load pre-trained GloVe embeddings\n","glove_model = api.load(\"glove-wiki-gigaword-100\")\n","\n","# Get the vector representation of the word \"language\"\n","vector = glove_model['language']\n","print(\"Vector representation of 'language':\")\n","print(vector)\n","\n","# Find the most similar words to \"language\"\n","similar_words = glove_model.most_similar('language')\n","print(\"\\\\nMost similar words to 'language':\")\n","print(similar_words)"],"metadata":{"id":"YgWPYSJ5ZCL_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3.4 Introduction to BERT Embeddings\n"],"metadata":{"id":"KYxvpOJvaGcS"}},{"cell_type":"markdown","source":["3.4.3 Implementing BERT Embeddings in Python"],"metadata":{"id":"Qn0r74ubaMKL"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertModel\n","import torch\n","\n","# Load pre-trained BERT model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')\n","\n","# Sample text\n","text = \"Natural Language Processing is fascinating.\"\n","\n","# Tokenize the text\n","inputs = tokenizer(text, return_tensors='pt')\n","\n","# Generate BERT embeddings\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","\n","# Get the embeddings for the [CLS] token (representing the entire input text)\n","cls_embeddings = outputs.last_hidden_state[:, 0, :]\n","\n","print(\"BERT Embeddings for the text:\")\n","print(cls_embeddings)"],"metadata":{"id":"_vhzoRNgaNsF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3.4.4 Fine-tuning BERT for Specific Tasks"],"metadata":{"id":"jEc27lJTaRNk"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n","from sklearn.model_selection import train_test_split\n","import torch\n","\n","# Sample text corpus and labels\n","documents = [\n","    \"Natural Language Processing is fascinating.\",\n","    \"Machine learning models are essential for AI.\",\n","    \"I love learning about deep learning.\",\n","    \"NLP and AI are closely related fields.\",\n","    \"Artificial Intelligence is transforming industries.\"\n","]\n","labels = [1, 0, 1, 1, 0]  # 1 for NLP-related, 0 for AI-related\n","\n","# Load pre-trained BERT tokenizer and model for sequence classification\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","\n","# Tokenize the text data\n","inputs = tokenizer(documents, padding=True, truncation=True, return_tensors='pt')\n","\n","# Create a dataset class\n","class TextDataset(torch.utils.data.Dataset):\n","    def __init__(self, inputs, labels):\n","        self.inputs = inputs\n","        self.labels = torch.tensor(labels)\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx] for key, val in self.inputs.items()}\n","        item['labels'] = self.labels[idx]\n","        return item\n","\n","# Split the data into training and testing sets\n","# Split inputs and labels separately\n","train_indices, test_indices = train_test_split(range(len(documents)), test_size=0.2, random_state=42)\n","\n","train_inputs = {key: inputs[key][train_indices] for key in inputs.keys()}\n","test_inputs = {key: inputs[key][test_indices] for key in inputs.keys()}\n","train_labels = [labels[i] for i in train_indices]\n","test_labels = [labels[i] for i in test_indices]\n","\n","\n","train_dataset = TextDataset(train_inputs, train_labels)\n","test_dataset = TextDataset(test_inputs, test_labels)\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    warmup_steps=10,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=10,\n",")\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n",")\n","\n","# Train the model\n","trainer.train()\n","\n","# Evaluate the model\n","results = trainer.evaluate()\n","print(\"Evaluation results:\")\n","print(results)"],"metadata":{"id":"Pj5qKIn_aR-I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Chapter-3 Assignments"],"metadata":{"id":"oEChii1Jae1n"}},{"cell_type":"markdown","source":["Exercise 1: Bag of Words"],"metadata":{"id":"6kr9xEpBaj5h"}},{"cell_type":"code","source":["documents = [\n","    \"Text processing is important for NLP.\",\n","    \"Bag of Words is a simple text representation method.\",\n","    \"Feature engineering is essential in machine learning.\"\n","]"],"metadata":{"id":"9f1GWctCan4D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Sample text corpus\n","documents = [\n","    \"Text processing is important for NLP.\",\n","    \"Bag of Words is a simple text representation method.\",\n","    \"Feature engineering is essential in machine learning.\"\n","]\n","\n","# Initialize the CountVectorizer\n","vectorizer = CountVectorizer()\n","\n","# Transform the text data\n","X = vectorizer.fit_transform(documents)\n","\n","# Convert the result to an array\n","bow_array = X.toarray()\n","\n","# Get the feature names (vocabulary)\n","vocab = vectorizer.get_feature_names_out()\n","\n","print(\"Vocabulary:\")\n","print(vocab)\n","\n","print(\"\\nBag of Words Array:\")\n","print(bow_array)"],"metadata":{"id":"TQCAPPdVa6So"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  \n"],"metadata":{"id":"lyfw4x2ibuCu"}},{"cell_type":"markdown","source":["Exercise 2: TF-IDF"],"metadata":{"id":"JKkhULhpa9dT"}},{"cell_type":"code","source":["documents = [\n","    \"Natural language processing is fun.\",\n","    \"Language models are important in NLP.\",\n","    \"Machine learning and NLP are closely related.\"\n","]"],"metadata":{"id":"LJFk0O-qa_Jx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Sample text corpus\n","documents = [\n","    \"Natural language processing is fun.\",\n","    \"Language models are important in NLP.\",\n","    \"Machine learning and NLP are closely related.\"\n","]\n","\n","# Initialize the TfidfVectorizer\n","vectorizer = TfidfVectorizer()\n","\n","# Transform the text data\n","X = vectorizer.fit_transform(documents)\n","\n","# Convert the result to an array\n","tfidf_array = X.toarray()\n","\n","# Get the feature names (vocabulary)\n","vocab = vectorizer.get_feature_names_out()\n","\n","print(\"Vocabulary:\")\n","print(vocab)\n","\n","print(\"\\nTF-IDF Array:\")\n","print(tfidf_array)"],"metadata":{"id":"LNEw-dwMbCdK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  \n"],"metadata":{"id":"pstovR0obvTq"}},{"cell_type":"markdown","source":["Exercise 3: Word2Vec"],"metadata":{"id":"lY84-FgebKR7"}},{"cell_type":"code","source":["text = \"Natural language processing is fun and exciting. Language models are important in NLP. Machine learning and NLP are closely related.\""],"metadata":{"id":"VATJhHEKbMJX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","import nltk\n","nltk.download('punkt')\n","\n","# Sample text corpus\n","text = \"Natural language processing is fun and exciting. Language models are important in NLP. Machine learning and NLP are closely related.\"\n","\n","# Tokenize the text into sentences\n","sentences = sent_tokenize(text)\n","\n","# Tokenize each sentence into words\n","tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n","\n","# Train a Word2Vec model using the Skip-Gram method\n","model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, sg=1, min_count=1)\n","\n","# Get the vector representation of the word \"NLP\"\n","vector = model.wv['NLP']\n","print(\"Vector representation of 'NLP':\")\n","print(vector)"],"metadata":{"id":"kQ4wQ1wjbNoW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  \n"],"metadata":{"id":"ZfaNdo89bwc6"}},{"cell_type":"markdown","source":["Exercise 4: GloVe"],"metadata":{"id":"RGHngQqlbPVi"}},{"cell_type":"code","source":["import gensim.downloader as api\n","\n","# Load pre-trained GloVe embeddings\n","glove_model = api.load(\"glove-wiki-gigaword-100\")\n","\n","# Get the vector representation of the word \"machine\"\n","vector = glove_model['machine']\n","print(\"Vector representation of 'machine':\")\n","print(vector)\n","\n","# Find the most similar words to \"machine\"\n","similar_words = glove_model.most_similar('machine')\n","print(\"\\nMost similar words to 'machine':\")\n","print(similar_words)"],"metadata":{"id":"Au42cQ9bbP_o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  \n"],"metadata":{"id":"RzwQbvTTbxaK"}},{"cell_type":"markdown","source":["Exercise 5: BERT Embeddings"],"metadata":{"id":"hoM5dMMobSkv"}},{"cell_type":"code","source":["text = \"Transformers are powerful models for NLP tasks.\""],"metadata":{"id":"nYjRWhzVbTGx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizer, BertModel\n","import torch\n","\n","# Load pre-trained BERT model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')\n","\n","# Sample text\n","text = \"Transformers are powerful models for NLP tasks.\"\n","\n","# Tokenize the text\n","inputs = tokenizer(text, return_tensors='pt')\n","\n","# Generate BERT embeddings\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","\n","# Get the embeddings for the [CLS] token (representing the entire input text)\n","cls_embeddings = outputs.last_hidden_state[:, 0, :]\n","\n","print(\"BERT Embeddings for the text:\")\n","print(cls_embeddings)"],"metadata":{"id":"mxGRMXeHbVm_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  \n"],"metadata":{"id":"4Po0hLnYbyxS"}}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMfQJiBjbA8WXlMTARrMmMa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 2.1 Understanding Text Data"],"metadata":{"id":"coAy5rDTFEAA"}},{"cell_type":"markdown","source":["2.1.3 Example: Exploring Raw Text Data"],"metadata":{"id":"ZIPt0DmcFact"}},{"cell_type":"code","source":["# Sample text\n","text = \"Natural Language Processing (NLP) enables computers to understand human language.\"\n","\n","# Display the text\n","print(\"Original Text:\")\n","print(text)\n","\n","# Length of the text\n","print(\"\\\\nLength of the text:\", len(text))\n","\n","# Unique characters in the text\n","unique_characters = set(text)\n","print(\"\\\\nUnique characters:\", unique_characters)\n","\n","# Number of words in the text\n","words = text.split()\n","print(\"\\\\nNumber of words:\", len(words))\n","\n","# Display the words\n","print(\"\\\\nWords in the text:\")\n","print(words)"],"metadata":{"id":"XyikXBjoFbSU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.1.5 Practical Example: Basic Text Preprocessing Steps"],"metadata":{"id":"_Lsq3-dPFpAT"}},{"cell_type":"code","source":["import string\n","\n","# Sample text\n","text = \"Natural Language Processing (NLP) enables computers to understand human language.\"\n","\n","# Convert to lowercase\n","text = text.lower()\n","print(\"Lowercased Text:\")\n","print(text)\n","\n","# Remove punctuation\n","text = text.translate(str.maketrans('', '', string.punctuation))\n","print(\"\\\\nText without Punctuation:\")\n","print(text)\n","\n","# Tokenize the text\n","tokens = text.split()\n","print(\"\\\\nTokens:\")\n","print(tokens)"],"metadata":{"id":"9dzP-tYEFpxR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.2 Text Cleaning: Stop Word Removal, Stemming, Lemmatization"],"metadata":{"id":"RpA5OAJYFtjK"}},{"cell_type":"markdown","source":["2.2.1 Stop Word Removal"],"metadata":{"id":"RD8Fm-SyF0O6"}},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","# Sample text\n","text = \"Natural Language Processing enables computers to understand human language.\"\n","\n","# Tokenize the text\n","tokens = text.split()\n","\n","# Remove stop words\n","stop_words = set(stopwords.words('english'))\n","filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n","\n","print(\"Original Tokens:\")\n","print(tokens)\n","\n","print(\"\\\\nFiltered Tokens:\")\n","print(filtered_tokens)"],"metadata":{"id":"k2SV_zAAF1Ly"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.2.2 Stemming"],"metadata":{"id":"6JpagWfoF4IA"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","\n","# Sample text\n","text = \"Natural Language Processing enables computers to understand human language.\"\n","\n","# Tokenize the text\n","tokens = text.split()\n","\n","# Initialize the stemmer\n","stemmer = PorterStemmer()\n","\n","# Stem the tokens\n","stemmed_tokens = [stemmer.stem(word) for word in tokens]\n","\n","print(\"Original Tokens:\")\n","print(tokens)\n","\n","print(\"\\\\nStemmed Tokens:\")\n","print(stemmed_tokens)"],"metadata":{"id":"q8R8o5DiF6oL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.2.3 Lemmatization"],"metadata":{"id":"fpXXB5KbF8M2"}},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","# Sample text\n","text = \"Natural Language Processing enables computers to understand human language.\"\n","\n","# Tokenize the text\n","tokens = text.split()\n","\n","# Initialize the lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","# Lemmatize the tokens\n","lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","print(\"Original Tokens:\")\n","print(tokens)\n","\n","print(\"\\\\nLemmatized Tokens:\")\n","print(lemmatized_tokens)"],"metadata":{"id":"f5iQ2PJ9F91T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.2.4 Practical Example: Combining Text Cleaning Techniques"],"metadata":{"id":"l96kJJI9GAKm"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Sample text\n","text = \"Natural Language Processing enables computers to understand human language.\"\n","\n","# Convert to lowercase\n","text = text.lower()\n","\n","# Remove punctuation\n","import string\n","text = text.translate(str.maketrans('', '', string.punctuation))\n","\n","# Tokenize the text\n","tokens = text.split()\n","\n","# Remove stop words\n","stop_words = set(stopwords.words('english'))\n","filtered_tokens = [word for word in tokens if word not in stop_words]\n","\n","# Initialize the stemmer and lemmatizer\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","\n","# Stem and lemmatize the filtered tokens\n","processed_tokens = [lemmatizer.lemmatize(stemmer.stem(word)) for word in filtered_tokens]\n","\n","print(\"Original Text:\")\n","print(text)\n","\n","print(\"\\\\nFiltered Tokens (Stop Words Removed):\")\n","print(filtered_tokens)\n","\n","print(\"\\\\nProcessed Tokens (Stemmed and Lemmatized):\")\n","print(processed_tokens)"],"metadata":{"id":"BeoA8u9iGDdq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.3 Regular Expressions"],"metadata":{"id":"f632K_sXGONh"}},{"cell_type":"markdown","source":["2.3.1 Basics of Regular Expressions"],"metadata":{"id":"mSte4bHTGSDv"}},{"cell_type":"code","source":["import re\n","\n","# Sample text\n","text = \"The quick brown fox jumps over the lazy dog.\"\n","\n","# Define a pattern to search for the word \"fox\"\n","pattern = r\"fox\"\n","\n","# Use re.search() to find the pattern in the text\n","match = re.search(pattern, text)\n","\n","# Display the match\n","if match:\n","    print(\"Match found:\", match.group())\n","else:\n","    print(\"No match found.\")"],"metadata":{"id":"rwCElGAjGRvr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.3.3 Practical Examples of Regex in Python"],"metadata":{"id":"xrFt-hC9GWia"}},{"cell_type":"code","source":["import re\n","\n","# Sample text with email addresses\n","text = \"Please contact us at support@example.com or sales@example.com for further information.\"\n","\n","# Define a regex pattern to match email addresses\n","pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\"\n","\n","# Use re.findall() to find all matches\n","emails = re.findall(pattern, text)\n","\n","# Display the extracted email addresses\n","print(\"Extracted Email Addresses:\")\n","print(emails)"],"metadata":{"id":"S44C9VY5GZHO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","# Sample text with phone numbers\n","text = \"Contact us at (123) 456-7890 or (987) 654-3210.\"\n","\n","# Define a regex pattern to match phone numbers\n","pattern = r\"\\(\\d{3}\\) \\d{3}-\\d{4}\"\n","\n","# Use re.findall() to find all matches\n","phone_numbers = re.findall(pattern, text)\n","\n","# Display the extracted phone numbers\n","print(\"Extracted Phone Numbers:\")\n","print(phone_numbers)"],"metadata":{"id":"Lj_uxeffVrln"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","# Sample text\n","text = \"The quick brown fox jumps over the lazy dog. The fox is clever.\"\n","\n","# Define a pattern to match the word \"fox\"\n","pattern = r\"fox\"\n","\n","# Use re.sub() to replace \"fox\" with \"cat\"\n","new_text = re.sub(pattern, \"cat\", text)\n","\n","# Display the modified text\n","print(\"Modified Text:\")\n","print(new_text)"],"metadata":{"id":"Vz5mlv2dGg9S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.3.4 Advanced Regex Techniques"],"metadata":{"id":"iklHSOb0GjDv"}},{"cell_type":"code","source":["import re\n","\n","# Sample text with dates\n","text = \"The event is scheduled for 2022-08-15. Another event is on 15/08/2022.\"\n","\n","# Define a regex pattern to match dates\n","pattern = r\"\\d{4}-\\d{2}-\\d{2}|\\d{2}/\\d{2}/\\d{4}\"\n","\n","# Use re.findall() to find all matches\n","dates = re.findall(pattern, text)\n","\n","# Display the extracted dates\n","print(\"Extracted Dates:\")\n","print(dates)"],"metadata":{"id":"LBarilVcGkzv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","# Sample text with hashtags\n","text = \"Loving the new features of this product! #excited #newrelease #tech\"\n","\n","# Define a regex pattern to match hashtags\n","pattern = r\"#\\w+\"\n","\n","# Use re.findall() to find all matches\n","hashtags = re.findall(pattern, text)\n","\n","# Display the extracted hashtags\n","print(\"Extracted Hashtags:\")\n","print(hashtags)"],"metadata":{"id":"kJa8wC1QGmlB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.4 Tokenization"],"metadata":{"id":"YiBqrjuDV_ro"}},{"cell_type":"markdown","source":["2.4.3 Word Tokenization"],"metadata":{"id":"aQHka1EOWGJd"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('punkt_tab') # Add this line to download the missing resource\n","from nltk.tokenize import word_tokenize\n","\n","# Sample text\n","text = \"Natural Language Processing enables computers to understand human language.\"\n","\n","# Perform word tokenization\n","tokens = word_tokenize(text)\n","\n","print(\"Word Tokens:\")\n","print(tokens)"],"metadata":{"id":"mTJxZwaHWHE5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","\n","# Load SpaCy model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Sample text\n","text = \"Natural Language Processing enables computers to understand human language.\"\n","\n","# Perform word tokenization\n","doc = nlp(text)\n","tokens = [token.text for token in doc]\n","\n","print(\"Word Tokens:\")\n","print(tokens)"],"metadata":{"id":"Ag_3FOX6WKcw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.4.4 Sentence Tokenization"],"metadata":{"id":"6aos956fWdTI"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize\n","\n","# Sample text\n","text = \"Natural Language Processing enables computers to understand human language. It is a fascinating field.\"\n","\n","# Perform sentence tokenization\n","sentences = sent_tokenize(text)\n","\n","print(\"Sentences:\")\n","print(sentences)"],"metadata":{"id":"OJeLSkKTWfW1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","\n","# Load SpaCy model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Sample text\n","text = \"Natural Language Processing enables computers to understand human language. It is a fascinating field.\"\n","\n","# Perform sentence tokenization\n","doc = nlp(text)\n","sentences = [sent.text for sent in doc.sents]\n","\n","print(\"Sentences:\")\n","print(sentences)"],"metadata":{"id":"q5JuLnP6WjR-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.4.5 Character Tokenization"],"metadata":{"id":"I6yLWSqbWkhb"}},{"cell_type":"code","source":["# Sample text\n","text = \"Natural Language Processing\"\n","\n","# Perform character tokenization\n","characters = list(text)\n","\n","print(\"Characters:\")\n","print(characters)"],"metadata":{"id":"BvTMg3gFWmSa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.4.6 Practical Example: Tokenization Pipeline"],"metadata":{"id":"C8Uzpg_3Wr73"}},{"cell_type":"code","source":["import nltk\n","import spacy\n","nltk.download('punkt')\n","\n","# Load SpaCy model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Sample text\n","text = \"Natural Language Processing enables computers to understand human language. It is a fascinating field.\"\n","\n","# Perform word tokenization using NLTK\n","word_tokens = nltk.word_tokenize(text)\n","print(\"Word Tokens:\")\n","print(word_tokens)\n","\n","# Perform sentence tokenization using NLTK\n","sentence_tokens = nltk.sent_tokenize(text)\n","print(\"\\\\nSentence Tokens:\")\n","print(sentence_tokens)\n","\n","# Perform sentence tokenization using SpaCy\n","doc = nlp(text)\n","spacy_sentence_tokens = [sent.text for sent in doc.sents]\n","print(\"\\\\nSentence Tokens (SpaCy):\")\n","print(spacy_sentence_tokens)\n","\n","# Perform word tokenization using SpaCy\n","spacy_word_tokens = [token.text for token in doc]\n","print(\"\\\\nWord Tokens (SpaCy):\")\n","print(spacy_word_tokens)\n","\n","# Perform character tokenization\n","char_tokens = list(text)\n","print(\"\\\\nCharacter Tokens:\")\n","print(char_tokens)"],"metadata":{"id":"9aEBa3eZWpnq"},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNZt6bqXubjLsuEtaFqlmrF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 6.1 Rule-Based Approaches"],"metadata":{"id":"sA0gTW9rhVSw"}},{"cell_type":"markdown","source":["**Installed the required Python prerequisite packages and libraries.**\n"],"metadata":{"id":"Gqxieku8yOeT"}},{"cell_type":"code","source":["!pip install textblob\n","!pip install afinn\n","!pip install tensorflow\n","!pip install keras\n","!pip install --upgrade transformers huggingface_hub\n","!pip install torch"],"metadata":{"id":"rdau6IjBhZJs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6.1.2 Implementing Rule-Based Sentiment Analysis"],"metadata":{"id":"to-kMIgqhXlO"}},{"cell_type":"code","source":["from textblob import TextBlob\n","\n","# Sample text\n","text = \"I love this product! It works wonderfully and the quality is excellent.\"\n","\n","# Perform sentiment analysis\n","blob = TextBlob(text)\n","sentiment = blob.sentiment\n","\n","print(\"Sentiment Analysis:\")\n","print(f\"Polarity: {sentiment.polarity}, Subjectivity: {sentiment.subjectivity}\")"],"metadata":{"id":"pjPFL6mhhbDM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6.1.3 Creating Custom Rule-Based Sentiment Analyzers"],"metadata":{"id":"hdY451Eehd99"}},{"cell_type":"code","source":["from afinn import Afinn\n","\n","# Initialize the Afinn sentiment analyzer\n","afinn = Afinn()\n","\n","# Sample text\n","text = \"I hate the traffic in this city. It makes commuting a nightmare.\"\n","\n","# Perform sentiment analysis\n","sentiment_score = afinn.score(text)\n","\n","# Determine sentiment based on score\n","if sentiment_score > 0:\n","    sentiment = \"Positive\"\n","elif sentiment_score < 0:\n","    sentiment = \"Negative\"\n","else:\n","    sentiment = \"Neutral\"\n","\n","print(\"Sentiment Analysis:\")\n","print(f\"Text: {text}\")\n","print(f\"Sentiment Score: {sentiment_score}\")\n","print(f\"Sentiment: {sentiment}\")"],"metadata":{"id":"TPxpjlgVhjwB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6.2 Machine Learning Approaches"],"metadata":{"id":"78GxBJjWhosH"}},{"cell_type":"markdown","source":["6.2.2 Feature Extraction"],"metadata":{"id":"YPADIZwFhuT-"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Sample text corpus\n","corpus = [\n","    \"I love this product! It's amazing.\",\n","    \"This is the worst service I have ever experienced.\",\n","    \"I am very happy with my purchase.\",\n","    \"I am disappointed with the quality of this item.\"\n","]\n","\n","# Initialize the TF-IDF Vectorizer\n","vectorizer = TfidfVectorizer()\n","\n","# Transform the text data into TF-IDF features\n","X = vectorizer.fit_transform(corpus)\n","\n","print(\"TF-IDF Feature Matrix:\")\n","print(X.toarray())"],"metadata":{"id":"GISLGGuthvi1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6.2.3 Model Training"],"metadata":{"id":"4cgQ7GX7hxRD"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Sample text corpus and labels\n","corpus = [\n","    \"I love this product! It's amazing.\",\n","    \"This is the worst service I have ever experienced.\",\n","    \"I am very happy with my purchase.\",\n","    \"I am disappointed with the quality of this item.\"\n","]\n","labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n","\n","# Transform the text data into TF-IDF features\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(corpus)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=42)\n","\n","# Initialize and train the Logistic Regression model\n","model = LogisticRegression()\n","model.fit(X_train, y_train)\n","\n","# Predict the sentiment of the test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","print(f\"Accuracy: {accuracy}\")\n","print(\"Classification Report:\")\n","print(report)"],"metadata":{"id":"FNV4cGh8hyc5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6.2.4 Evaluating Machine Learning Models"],"metadata":{"id":"cZzQ6cTgh1rq"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Predict the sentiment of the test set\n","y_pred = model.predict(X_test)\n","\n","# Calculate evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(f\"Accuracy: {accuracy}\")\n","print(f\"Precision: {precision}\")\n","print(f\"Recall: {recall}\")\n","print(f\"F1 Score: {f1}\")"],"metadata":{"id":"yV_D-bDjh2Sa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6.3 Deep Learning Approaches"],"metadata":{"id":"WklxblQBh4J8"}},{"cell_type":"markdown","source":["6.3.2 Convolutional Neural Networks (CNNs)"],"metadata":{"id":"DIgRhCJRh77I"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Embedding\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","# Sample text corpus and labels\n","corpus = [\n","    \"I love this product! It's amazing.\",\n","    \"This is the worst service I have ever experienced.\",\n","    \"I am very happy with my purchase.\",\n","    \"I am disappointed with the quality of this item.\"\n","]\n","labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n","\n","# Tokenize and pad the text data\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(corpus)\n","X = tokenizer.texts_to_sequences(corpus)\n","X = pad_sequences(X, maxlen=10)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=42)\n","\n","# Define the CNN model\n","model = Sequential()\n","model.add(Embedding(input_dim=5000, output_dim=50, input_length=10))\n","model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n","model.add(GlobalMaxPooling1D())\n","model.add(Dense(10, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","# The error is likely here, due to the data type of X_train\n","# Convert y_train and y_test to numpy arrays\n","model.fit(X_train, np.array(y_train), epochs=5, verbose=1, validation_data=(X_test, np.array(y_test)))\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X_test, np.array(y_test))\n","print(f\"Accuracy: {accuracy}\")\n","\n","# Predict the sentiment of new text\n","new_text = [\"The product is excellent and I love it.\"]\n","new_text_seq = tokenizer.texts_to_sequences(new_text)\n","new_text_padded = pad_sequences(new_text_seq, maxlen=10)\n","prediction = model.predict(new_text_padded)\n","print(\"Prediction:\", \"Positive\" if prediction[0][0] > 0.5 else \"Negative\")"],"metadata":{"id":"SZItF2rSiGup"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6.3.3 Recurrent Neural Networks (RNNs) and Long Short-Term Memory Networks (LSTMs)\n"],"metadata":{"id":"QH4rJXU_iLpF"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM, Embedding\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","# Sample text corpus and labels\n","corpus = [\n","    \"I love this product! It's amazing.\",\n","    \"This is the worst service I have ever experienced.\",\n","    \"I am very happy with my purchase.\",\n","    \"I am disappointed with the quality of this item.\"\n","]\n","labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n","\n","# Tokenize and pad the text data\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(corpus)\n","X = tokenizer.texts_to_sequences(corpus)\n","X = pad_sequences(X, maxlen=10)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=42)\n","\n","# Define the LSTM model\n","model = Sequential()\n","model.add(Embedding(input_dim=5000, output_dim=50, input_length=10))\n","model.add(LSTM(100))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","# Convert y_train and y_test to numpy arrays to avoid the ValueError\n","model.fit(X_train, np.array(y_train), epochs=5, verbose=1, validation_data=(X_test, np.array(y_test)))\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X_test, np.array(y_test))\n","print(f\"Accuracy: {accuracy}\")\n","\n","# Predict the sentiment of new text\n","new_text = [\"The product is excellent and I love it.\"]\n","new_text_seq = tokenizer.texts_to_sequences(new_text)\n","new_text_padded = pad_sequences(new_text_seq, maxlen=10)\n","prediction = model.predict(new_text_padded)\n","print(\"Prediction:\", \"Positive\" if prediction[0][0] > 0.5 else \"Negative\")"],"metadata":{"id":"vHgJ3vjGiMjh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6.3.4 Transformer-Based Models"],"metadata":{"id":"SLFtva59iQD1"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from transformers import BertTokenizer\n","from sklearn.model_selection import train_test_split\n","\n","# Sample text corpus and labels\n","corpus = [\n","    \"I love this product! It's amazing.\",\n","    \"This is the worst service I have ever experienced.\",\n","    \"I am very happy with my purchase.\",\n","    \"I am disappointed with the quality of this item.\"\n","]\n","labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n","\n","# Initialize the BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenize and encode the text data\n","max_length = 10\n","X = tokenizer(corpus, padding='max_length', truncation=True, max_length=max_length, return_tensors='np')\n","input_ids = X['input_ids']\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(input_ids, labels, test_size=0.25, random_state=42)\n","\n","# Build a simple Keras model\n","vocab_size = tokenizer.vocab_size\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=32, input_length=max_length),\n","    tf.keras.layers.GlobalAveragePooling1D(),\n","    tf.keras.layers.Dense(16, activation='relu'),\n","    tf.keras.layers.Dense(2, activation='softmax')\n","])\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train, np.array(y_train), epochs=10, batch_size=2, validation_data=(X_test, np.array(y_test)))\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X_test, np.array(y_test))\n","print(f\"Accuracy: {accuracy}\")\n","\n","# Predict the sentiment of new text\n","new_text = [\"The product is excellent and I love it.\"]\n","new_text_enc = tokenizer(new_text, padding='max_length', truncation=True, max_length=max_length, return_tensors='np')\n","prediction = model.predict(new_text_enc['input_ids'])\n","print(\"Prediction:\", \"Positive\" if np.argmax(prediction) == 1 else \"Negative\")\n"],"metadata":{"id":"vVrvJ6cCiVeR"},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOYcyv+gZQFsRiDSHjvb8FC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 9.1 Sequence to Sequence Models"],"metadata":{"id":"OLwjo27tyv1U"}},{"cell_type":"markdown","source":["9.1.2 Implementing a Basic Seq2Seq Model\n"],"metadata":{"id":"piorIXC2yx1e"}},{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"id":"Ignsyu4ry0JM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Dense\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Sample data\n","input_texts = [\n","    \"Hello.\",\n","    \"How are you?\",\n","    \"What is your name?\",\n","    \"Good morning.\",\n","    \"Good night.\"\n","]\n","\n","target_texts = [\n","    \"Bonjour.\",\n","    \"Comment ça va?\",\n","    \"Quel est votre nom?\",\n","    \"Bonjour.\",\n","    \"Bonne nuit.\"\n","]\n","\n","# Tokenize the data\n","input_tokenizer = Tokenizer()\n","input_tokenizer.fit_on_texts(input_texts)\n","input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n","input_maxlen = max(len(seq) for seq in input_sequences)\n","input_vocab_size = len(input_tokenizer.word_index) + 1\n","\n","target_tokenizer = Tokenizer()\n","target_tokenizer.fit_on_texts(target_texts)\n","target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n","target_maxlen = max(len(seq) for seq in target_sequences)\n","target_vocab_size = len(target_tokenizer.word_index) + 1\n","\n","# Pad sequences\n","input_sequences = pad_sequences(input_sequences, maxlen=input_maxlen, padding='post')\n","target_sequences = pad_sequences(target_sequences, maxlen=target_maxlen, padding='post')\n","\n","# Split target sequences into input and output sequences\n","target_input_sequences = target_sequences[:, :-1]\n","target_output_sequences = target_sequences[:, 1:]\n","\n","# Build the Seq2Seq model\n","latent_dim = 256\n","\n","# Encoder\n","encoder_inputs = Input(shape=(input_maxlen,))\n","encoder_embedding = tf.keras.layers.Embedding(input_vocab_size, latent_dim)(encoder_inputs)\n","encoder_lstm = LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n","encoder_states = [state_h, state_c]\n","\n","# Decoder\n","decoder_inputs = Input(shape=(None,))\n","decoder_embedding = tf.keras.layers.Embedding(target_vocab_size, latent_dim)(decoder_inputs)\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n","decoder_dense = Dense(target_vocab_size, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the model\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n","\n","# Train the model\n","model.fit([input_sequences, target_input_sequences], target_output_sequences,\n","          batch_size=64, epochs=100, validation_split=0.2)\n","\n","# Inference models for translation\n","# Encoder model\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","# Decoder model\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_outputs, state_h, state_c = decoder_lstm(\n","    decoder_embedding, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs] + decoder_states)\n","\n","# Function to decode the sequence\n","def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1))\n","\n","    # Populate the first token of target sequence with the start token.\n","    target_seq[0, 0] = target_tokenizer.word_index['bonjour']\n","\n","    # Sampling loop for a batch of sequences\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_word = target_tokenizer.index_word[sampled_token_index]\n","        decoded_sentence += ' ' + sampled_word\n","\n","        # Exit condition: either hit max length or find stop token.\n","        if (sampled_word == '.' or\n","           len(decoded_sentence) > target_maxlen):\n","            stop_condition = True\n","\n","        # Update the target sequence (length 1).\n","        target_seq = np.zeros((1, 1))\n","        target_seq[0, 0] = sampled_token_index\n","\n","        # Update states\n","        states_value = [h, c]\n","\n","    return decoded_sentence\n","\n","# Test the model\n","for seq_index in range(5):\n","    input_seq = input_sequences[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-')\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)"],"metadata":{"id":"0P06bcb7y2Wx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 9.2 Attention Mechanisms"],"metadata":{"id":"4RMlHAusy5ZE"}},{"cell_type":"markdown","source":["9.2.3 Implementing Attention Mechanisms in Seq2Seq Models"],"metadata":{"id":"WfO4dVmRy_0h"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Concatenate, TimeDistributed\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Sample data\n","input_texts = [\n","    \"Hello.\",\n","    \"How are you?\",\n","    \"What is your name?\",\n","    \"Good morning.\",\n","    \"Good night.\"\n","]\n","\n","target_texts = [\n","    \"Bonjour.\",\n","    \"Comment ça va?\",\n","    \"Quel est votre nom?\",\n","    \"Bonjour.\",\n","    \"Bonne nuit.\"\n","]\n","\n","# Tokenize the data\n","input_tokenizer = Tokenizer()\n","input_tokenizer.fit_on_texts(input_texts)\n","input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n","input_maxlen = max(len(seq) for seq in input_sequences)\n","input_vocab_size = len(input_tokenizer.word_index) + 1\n","\n","target_tokenizer = Tokenizer()\n","target_tokenizer.fit_on_texts(target_texts)\n","target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n","target_maxlen = max(len(seq) for seq in target_sequences)\n","target_vocab_size = len(target_tokenizer.word_index) + 1\n","\n","# Pad sequences\n","input_sequences = pad_sequences(input_sequences, maxlen=input_maxlen, padding='post')\n","target_sequences = pad_sequences(target_sequences, maxlen=target_maxlen, padding='post')\n","\n","# Split target sequences into input and output sequences\n","target_input_sequences = target_sequences[:, :-1]\n","target_output_sequences = target_sequences[:, 1:]\n","\n","# Define the Seq2Seq model with Attention\n","latent_dim = 256\n","\n","# Encoder\n","encoder_inputs = Input(shape=(input_maxlen,))\n","encoder_embedding = Embedding(input_vocab_size, latent_dim)(encoder_inputs)\n","encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n","encoder_states = [state_h, state_c]\n","\n","# Decoder\n","decoder_inputs = Input(shape=(None,))\n","decoder_embedding = Embedding(target_vocab_size, latent_dim)(decoder_inputs)\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n","\n","# Attention mechanism\n","attention = tf.keras.layers.Attention()([decoder_outputs, encoder_outputs])\n","decoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attention])\n","\n","# Dense layer to generate predictions\n","decoder_dense = TimeDistributed(Dense(target_vocab_size, activation='softmax'))\n","decoder_outputs = decoder_dense(decoder_concat_input)\n","\n","# Define the model\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n","\n","# Train the model\n","model.fit([input_sequences, target_input_sequences], target_output_sequences,\n","          batch_size=64, epochs=100, validation_split=0.2)\n","\n","# Inference models for translation\n","# Encoder model\n","encoder_model = Model(encoder_inputs, [encoder_outputs] + encoder_states)\n","\n","# Decoder model\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_hidden_state_input = Input(shape=(input_maxlen, latent_dim))\n","decoder_outputs, state_h, state_c = decoder_lstm(\n","    decoder_embedding, initial_state=decoder_states_inputs)\n","attention_output = attention([decoder_outputs, decoder_hidden_state_input])\n","decoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attention_output])\n","decoder_outputs = decoder_dense(decoder_concat_input)\n","decoder_model = Model(\n","    [decoder_inputs] + [decoder_hidden_state_input] + decoder_states_inputs,\n","    [decoder_outputs] + [state_h, state_c])\n","\n","# Function to decode the sequence\n","def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    encoder_outputs, state_h, state_c = encoder_model.predict(input_seq)\n","    states_value = [state_h, state_c]\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1))\n","\n","    # Populate the first token of target sequence with the start token.\n","    target_seq[0, 0] = target_tokenizer.word_index['bonjour']\n","\n","    # Sampling loop for a batch of sequences\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + [encoder_outputs] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_word = target_tokenizer.index_word[sampled_token_index]\n","        decoded_sentence += ' ' + sampled_word\n","\n","        # Exit condition: either hit max length or find stop token.\n","        if (sampled_word == '.' or\n","           len(decoded_sentence) > target_maxlen):\n","            stop_condition = True\n","\n","        # Update the target sequence (length 1).\n","        target_seq = np.zeros((1, 1))\n","        target_seq[0, 0] = sampled_token_index\n","\n","        # Update states\n","        states_value = [h, c]\n","\n","    return decoded_sentence\n","\n","# Test the model\n","for seq_index in range(5):\n","    input_seq = input_sequences[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-')\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)"],"metadata":{"id":"auzzpHJCy9HX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 9.3 Transformer Models"],"metadata":{"id":"f4jlDMwczDxH"}},{"cell_type":"markdown","source":["9.3.3 Implementing Transformer Models in TensorFlow"],"metadata":{"id":"qsBhQpt7zHhE"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"9QZQaMdfzI-D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import T5ForConditionalGeneration, T5Tokenizer\n","\n","# Load the pre-trained T5 model and tokenizer\n","model_name = \"t5-small\"\n","model = T5ForConditionalGeneration.from_pretrained(model_name)\n","tokenizer = T5Tokenizer.from_pretrained(model_name)\n","\n","# Sample text\n","text = \"\"\"translate English to French: Machine learning is a subset of artificial intelligence. It involves algorithms and statistical models to perform tasks without explicit instructions. Machine learning is widely used in various applications such as image recognition, natural language processing, and autonomous driving. It relies on patterns and inference instead of predefined rules.\"\"\"\n","\n","# Tokenize and encode the text\n","inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True)\n","\n","# Generate the translation\n","output_ids = model.generate(inputs, max_length=150, num_beams=4, early_stopping=True)\n","translation = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","\n","print(\"Translation:\")\n","print(translation)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G04UnCGUzMEi","executionInfo":{"status":"ok","timestamp":1757395957952,"user_tz":300,"elapsed":4194,"user":{"displayName":"Jimmy Ardiansyah","userId":"06913112371375508078"}},"outputId":"6454e18c-16af-4a72-9813-bbda03ae0fbc"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Translation:\n","Dabei handelt es sich um eine Untergruppe der künstlichen Intelligenz, die Algorithmen und statistische Modelle zur Durchführung von Aufgaben ohne ausdrückliche Anweisungen einschließt.\n"]}]},{"cell_type":"markdown","source":["9.3.4 Example: Visualizing Self-Attention Scores"],"metadata":{"id":"qtvc7yWlzNms"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Function to visualize attention scores\n","def visualize_attention(model, tokenizer, text):\n","    inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True)\n","    outputs = model.generate(inputs, output_attentions=True)\n","    attentions = outputs[-1]  # Get the attention scores\n","\n","    # Convert to numpy array for visualization\n","    attention_matrix = attentions[-1][0][0].detach().numpy()\n","\n","    # Plot the attention scores\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(attention_matrix, cmap=\"viridis\")\n","    plt.title(\"Self-Attention Scores\")\n","    plt.xlabel(\"Input Tokens\")\n","    plt.ylabel(\"Output Tokens\")\n","    plt.show()\n","\n","# Visualize attention scores for a sample sentence\n","sample_text = \"translate English to French: How are you?\"\n","visualize_attention(model, tokenizer, sample_text)"],"metadata":{"id":"1lplfkvszOKX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Chapter-9 Assignments"],"metadata":{"id":"tXZxCe96zR54"}},{"cell_type":"markdown","source":["Exercise 1: Sequence to Sequence (Seq2Seq) Model with TensorFlow"],"metadata":{"id":"s-o8Wud8zWIu"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Sample data\n","input_texts = [\"Hello.\", \"How are you?\", \"What is your name?\", \"Good morning.\", \"Good night.\"]\n","target_texts = [\"Hola.\", \"¿Cómo estás?\", \"¿Cuál es tu nombre?\", \"Buenos días.\", \"Buenas noches.\"]\n","\n","# Tokenize the data\n","input_tokenizer = Tokenizer()\n","input_tokenizer.fit_on_texts(input_texts)\n","input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n","input_maxlen = max(len(seq) for seq in input_sequences)\n","input_vocab_size = len(input_tokenizer.word_index) + 1\n","\n","target_tokenizer = Tokenizer()\n","target_tokenizer.fit_on_texts(target_texts)\n","target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n","target_maxlen = max(len(seq) for seq in target_sequences)\n","target_vocab_size = len(target_tokenizer.word_index) + 1\n","\n","# Pad sequences\n","input_sequences = pad_sequences(input_sequences, maxlen=input_maxlen, padding='post')\n","target_sequences = pad_sequences(target_sequences, maxlen=target_maxlen, padding='post')\n","\n","# Split target sequences into input and output sequences\n","target_input_sequences = target_sequences[:, :-1]\n","target_output_sequences = target_sequences[:, 1:]\n","\n","# Build the Seq2Seq model\n","latent_dim = 256\n","\n","# Encoder\n","encoder_inputs = Input(shape=(input_maxlen,))\n","encoder_embedding = Embedding(input_vocab_size, latent_dim)(encoder_inputs)\n","encoder_lstm = LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n","encoder_states = [state_h, state_c]\n","\n","# Decoder\n","decoder_inputs = Input(shape=(None,))\n","decoder_embedding = Embedding(target_vocab_size, latent_dim)(decoder_inputs)\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n","decoder_dense = Dense(target_vocab_size, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the model\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n","\n","# Train the model\n","model.fit([input_sequences, target_input_sequences], target_output_sequences,\n","          batch_size=64, epochs=100, validation_split=0.2)\n","\n","# Inference models for translation\n","# Encoder model\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","# Decoder model\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_outputs, state_h, state_c = decoder_lstm(\n","    decoder_embedding, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs] + decoder_states)\n","\n","# Function to decode the sequence\n","def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1))\n","\n","    # Populate the first token of target sequence with the start token.\n","    # Check if 'hola' is in the target tokenizer's word index, otherwise use a default or handle appropriately.\n","    start_token_index = target_tokenizer.word_index.get('hola', None)\n","    if start_token_index is None:\n","        # Handle the case where 'hola' is not in the vocabulary,\n","        # maybe use a different start token or the most frequent word.\n","        # For this example, we'll just use the first word in the index_word dictionary if available,\n","        # or raise an error if the vocabulary is empty.\n","        if not target_tokenizer.index_word:\n","            raise ValueError(\"Target vocabulary is empty.\")\n","        start_token_index = list(target_tokenizer.index_word.keys())[0] # Use the index of the first word\n","\n","    target_seq[0, 0] = start_token_index\n","\n","    # Sampling loop for a batch of sequences\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","\n","        # Exit condition: either hit max length, find stop token, or sample padding token (index 0).\n","        if sampled_token_index == 0 or len(decoded_sentence) > target_maxlen:\n","             stop_condition = True\n","             continue # Skip adding padding token to the sentence\n","\n","        sampled_word = target_tokenizer.index_word[sampled_token_index]\n","        decoded_sentence += ' ' + sampled_word\n","\n","        if sampled_word == '.':\n","            stop_condition = True\n","\n","        # Update the target sequence (length 1).\n","        target_seq = np.zeros((1, 1))\n","        target_seq[0, 0] = sampled_token_index\n","\n","        # Update states\n","        states_value = [h, c]\n","\n","    return decoded_sentence.strip() # Remove leading/trailing whitespace\n","\n","# Test the model\n","for seq_index in range(5):\n","    input_seq = input_sequences[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-')\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)"],"metadata":{"id":"RO7KLgk8zX9s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"_MYOMZTu3T8p"}},{"cell_type":"markdown","source":["Exercise 2: Seq2Seq Model with Attention in TensorFlow"],"metadata":{"id":"1wShGDjazbRT"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Concatenate, TimeDistributed, Attention\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Sample data\n","input_texts = [\"Hello.\", \"How are you?\", \"What is your name?\", \"Good morning.\", \"Good night.\"]\n","target_texts = [\"Hola.\", \"¿Cómo estás?\", \"¿Cuál es tu nombre?\", \"Buenos días.\", \"Buenas noches.\"]\n","\n","# Tokenize the data\n","input_tokenizer = Tokenizer()\n","input_tokenizer.fit_on_texts(input_texts)\n","input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n","input_maxlen = max(len(seq) for seq in input_sequences)\n","input_vocab_size = len(input_tokenizer.word_index) + 1\n","\n","target_tokenizer = Tokenizer()\n","target_tokenizer.fit_on_texts(target_texts)\n","target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n","target_maxlen = max(len(seq) for seq in target_sequences)\n","target_vocab_size = len(target_tokenizer.word_index) + 1\n","\n","# Pad sequences\n","input_sequences = pad_sequences(input_sequences, maxlen=input_maxlen, padding='post')\n","target_sequences = pad_sequences(target_sequences, maxlen=target_maxlen, padding='post')\n","\n","# Split target sequences into input and output sequences\n","target_input_sequences = target_sequences[:, :-1]\n","target_output_sequences = target_sequences[:, 1:]\n","\n","# Define the Seq2Seq model with Attention\n","latent_dim = 256\n","\n","# Encoder\n","encoder_inputs = Input(shape=(input_maxlen,))\n","encoder_embedding = Embedding(input_vocab_size, latent_dim)(encoder_inputs)\n","encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n","encoder_states = [state_h, state_c]\n","\n","# Decoder\n","decoder_inputs = Input(shape=(None,))\n","decoder_embedding = Embedding(target_vocab_size, latent_dim)(decoder_inputs)\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n","\n","# Attention mechanism\n","attention_layer = Attention()\n","attention_output = attention_layer([decoder_outputs, encoder_outputs])\n","decoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attention_output])\n","\n","# Dense layer to generate predictions\n","decoder_dense = TimeDistributed(Dense(target_vocab_size, activation='softmax'))\n","decoder_outputs = decoder_dense(decoder_concat_input)\n","\n","# Define the model\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n","\n","# Train the model\n","model.fit([input_sequences, target_input_sequences], target_output_sequences,\n","          batch_size=64, epochs=100, validation_split=0.2)\n","\n","# Inference models for translation\n","# Encoder model\n","encoder_model = Model(encoder_inputs, [encoder_outputs] + encoder_states)\n","\n","# Decoder model\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_hidden_state_input = Input(shape=(input_maxlen, latent_dim))\n","decoder_outputs, state_h, state_c = decoder_lstm(\n","    decoder_embedding, initial_state=decoder_states_inputs)\n","attention_output = attention_layer([decoder_outputs, decoder_hidden_state_input])\n","decoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attention_output])\n","decoder_outputs = decoder_dense(decoder_concat_input)\n","decoder_model = Model(\n","    [decoder_inputs] + [decoder_hidden_state_input] + decoder_states_inputs,\n","    [decoder_outputs] + [state_h, state_c])\n","\n","# Function to decode the sequence\n","def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    encoder_outputs, state_h, state_c = encoder_model.predict(input_seq)\n","    states_value = [state_h, state_c]\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1))\n","\n","    # Populate the first token of target sequence with the start token.\n","    start_token_index = target_tokenizer.word_index.get('hola', None)\n","    if start_token_index is None:\n","        if not target_tokenizer.index_word:\n","            raise ValueError(\"Target vocabulary is empty.\")\n","        start_token_index = list(target_tokenizer.index_word.keys())[0]\n","\n","    target_seq[0, 0] = start_token_index\n","\n","\n","    # Sampling loop for a batch of sequences\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + [encoder_outputs] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","\n","        # Exit condition: either hit max length, find stop token, or sample padding token (index 0).\n","        if sampled_token_index == 0 or len(decoded_sentence) > target_maxlen:\n","             stop_condition = True\n","             continue # Skip adding padding token to the sentence\n","\n","        sampled_word = target_tokenizer.index_word[sampled_token_index]\n","        decoded_sentence += ' ' + sampled_word\n","\n","        # Exit condition: either hit max length or find stop token.\n","        if sampled_word == '.':\n","            stop_condition = True\n","\n","\n","        # Update the target sequence (length 1).\n","        target_seq = np.zeros((1, 1))\n","        target_seq[0, 0] = sampled_token_index\n","\n","        # Update states\n","        states_value = [h, c]\n","\n","    return decoded_sentence.strip() # Remove leading/trailing whitespace\n","\n","# Test the model\n","for seq_index in range(5):\n","    input_seq = input_sequences[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-')\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)"],"metadata":{"id":"zOVXY6oczdTf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"UBBJfa8B-WCu"}},{"cell_type":"markdown","source":["Exercise 3: Transformer Model with T5"],"metadata":{"id":"I_xj6GHpzhvX"}},{"cell_type":"code","source":["from transformers import T5ForConditionalGeneration, T5Tokenizer\n","\n","# Load the pre-trained T5 model and tokenizer\n","model_name = \"t5-small\"\n","model = T5ForConditionalGeneration.from_pretrained(model_name)\n","tokenizer = T5Tokenizer.from_pretrained(model_name)\n","\n","# Sample text\n","text = \"\"\"translate English to Spanish: Machine learning is a subset of artificial intelligence.\n","It involves algorithms and statistical models to perform tasks without explicit instructions.\n","Machine learning is widely used in various applications such as image recognition,\n","natural language processing, and autonomous driving.\n","It relies on patterns and inference instead of predefined rules.\"\"\"\n","\n","# Tokenize and encode the text\n","inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True)\n","\n","# Generate the translation\n","output_ids = model.generate(inputs, max_length=150, num_beams=4, early_stopping=True)\n","translation = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","\n","print(\"Translation:\")\n","print(translation)"],"metadata":{"id":"jYSLjLt9ziUW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"rAa1SAIZ-YMP"}},{"cell_type":"markdown","source":["Exercise 5: Comparing Seq2Seq, Attention, and Transformer Models"],"metadata":{"id":"xI0oi3sXzkGZ"}},{"cell_type":"code","source":["input_seq = pad_sequences(input_tokenizer.texts_to_sequences([\"How are you?\"]), maxlen=input_maxlen, padding='post')\n","seq2seq_translation = decode_sequence(input_seq)\n","print(\"Seq2Seq Translation:\", seq2seq_translation)"],"metadata":{"id":"zhBLVscDzmVU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_seq = pad_sequences(input_tokenizer.texts_to_sequences([\"How are you?\"]), maxlen=input_maxlen, padding='post')\n","attention_translation = decode_sequence(input_seq)\n","print(\"Seq2Seq with Attention Translation:\", attention_translation)"],"metadata":{"id":"JdGJgitqzqVO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"translate English to Spanish: How are you?\"\n","inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True)\n","output_ids = model.generate(inputs, max_length=50, num_beams=4, early_stopping=True)\n","transformer_translation = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","print(\"Transformer (T5) Translation:\", transformer_translation)"],"metadata":{"id":"Q5Z2FleQzsq6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["** Provide your feedback on each the Model **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"7_A6wNvh-Zzo"}}]}
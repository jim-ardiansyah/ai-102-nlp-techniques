{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMLbOKrVLpW32lnLy53dZLc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 9.1 Sequence to Sequence Models"],"metadata":{"id":"OLwjo27tyv1U"}},{"cell_type":"markdown","source":["**Installed the required Python prerequisite packages and libraries.**"],"metadata":{"id":"f07fLCTr-1vs"}},{"cell_type":"code","source":["!pip install tensorflow\n","!pip install keras\n","!pip install transformers"],"metadata":{"id":"Ignsyu4ry0JM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["9.1.2 Implementing a Basic Seq2Seq Model\n"],"metadata":{"id":"piorIXC2yx1e"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Dense\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Sample data\n","input_texts = [\n","    \"Hello.\",\n","    \"How are you?\",\n","    \"What is your name?\",\n","    \"Good morning.\",\n","    \"Good night.\"\n","]\n","\n","target_texts = [\n","    \"Bonjour.\",\n","    \"Comment ça va?\",\n","    \"Quel est votre nom?\",\n","    \"Bonjour.\",\n","    \"Bonne nuit.\"\n","]\n","\n","# Tokenize the data\n","input_tokenizer = Tokenizer()\n","input_tokenizer.fit_on_texts(input_texts)\n","input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n","input_maxlen = max(len(seq) for seq in input_sequences)\n","input_vocab_size = len(input_tokenizer.word_index) + 1\n","\n","target_tokenizer = Tokenizer()\n","target_tokenizer.fit_on_texts(target_texts)\n","target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n","target_maxlen = max(len(seq) for seq in target_sequences)\n","target_vocab_size = len(target_tokenizer.word_index) + 1\n","\n","# Pad sequences\n","input_sequences = pad_sequences(input_sequences, maxlen=input_maxlen, padding='post')\n","target_sequences = pad_sequences(target_sequences, maxlen=target_maxlen, padding='post')\n","\n","# Split target sequences into input and output sequences\n","target_input_sequences = target_sequences[:, :-1]\n","target_output_sequences = target_sequences[:, 1:]\n","\n","# Build the Seq2Seq model\n","latent_dim = 256\n","\n","# Encoder\n","encoder_inputs = Input(shape=(input_maxlen,))\n","encoder_embedding = tf.keras.layers.Embedding(input_vocab_size, latent_dim)(encoder_inputs)\n","encoder_lstm = LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n","encoder_states = [state_h, state_c]\n","\n","# Decoder\n","decoder_inputs = Input(shape=(None,))\n","decoder_embedding = tf.keras.layers.Embedding(target_vocab_size, latent_dim)(decoder_inputs)\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n","decoder_dense = Dense(target_vocab_size, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the model\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n","\n","# Train the model\n","model.fit([input_sequences, target_input_sequences], target_output_sequences,\n","          batch_size=64, epochs=100, validation_split=0.2)\n","\n","# Inference models for translation\n","# Encoder model\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","# Decoder model\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_outputs, state_h, state_c = decoder_lstm(\n","    decoder_embedding, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs] + decoder_states)\n","\n","# Function to decode the sequence\n","def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1))\n","\n","    # Populate the first token of target sequence with the start token.\n","    # Assuming 'bonjour' is the start token.\n","    # If not, you might need to add a dedicated start token to your vocabulary\n","    # and use its index here.\n","    if 'bonjour' in target_tokenizer.word_index:\n","      target_seq[0, 0] = target_tokenizer.word_index['bonjour']\n","    else:\n","      # Handle the case where 'bonjour' is not in the vocabulary\n","      # This might involve using a special start-of-sequence token index\n","      # or raising an error depending on your desired behavior.\n","      print(\"Warning: 'bonjour' not found in target vocabulary. Using index 1 as a fallback.\")\n","      target_seq[0, 0] = 1 # Using 1 as a fallback, adjust if needed\n","\n","\n","    # Sampling loop for a batch of sequences\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        # Skip the padding token (index 0)\n","        if sampled_token_index == 0:\n","            break\n","        sampled_word = target_tokenizer.index_word[sampled_token_index]\n","        decoded_sentence += ' ' + sampled_word\n","\n","        # Exit condition: either hit max length or find stop token.\n","        if (sampled_word == '.' or\n","           len(decoded_sentence.split()) > target_maxlen): # Check length of words instead of characters\n","            stop_condition = True\n","\n","        # Update the target sequence (length 1).\n","        target_seq = np.zeros((1, 1))\n","        target_seq[0, 0] = sampled_token_index\n","\n","        # Update states\n","        states_value = [h, c]\n","\n","    return decoded_sentence.strip() # Remove leading/trailing whitespace\n","\n","# Test the model\n","for seq_index in range(5):\n","    input_seq = input_sequences[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-')\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)"],"metadata":{"id":"0P06bcb7y2Wx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 9.2 Attention Mechanisms"],"metadata":{"id":"4RMlHAusy5ZE"}},{"cell_type":"markdown","source":["9.2.3 Implementing Attention Mechanisms in Seq2Seq Models"],"metadata":{"id":"WfO4dVmRy_0h"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Concatenate, TimeDistributed\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Sample data\n","input_texts = [\n","    \"Hello.\",\n","    \"How are you?\",\n","    \"What is your name?\",\n","    \"Good morning.\",\n","    \"Good night.\"\n","]\n","\n","target_texts = [\n","    \"Bonjour.\",\n","    \"Comment ça va?\",\n","    \"Quel est votre nom?\",\n","    \"Bonjour.\",\n","    \"Bonne nuit.\"\n","]\n","\n","# Tokenize the data\n","input_tokenizer = Tokenizer()\n","input_tokenizer.fit_on_texts(input_texts)\n","input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n","input_maxlen = max(len(seq) for seq in input_sequences)\n","input_vocab_size = len(input_tokenizer.word_index) + 1\n","\n","target_tokenizer = Tokenizer()\n","target_tokenizer.fit_on_texts(target_texts)\n","target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n","target_maxlen = max(len(seq) for seq in target_sequences)\n","target_vocab_size = len(target_tokenizer.word_index) + 1\n","\n","# Pad sequences\n","input_sequences = pad_sequences(input_sequences, maxlen=input_maxlen, padding='post')\n","target_sequences = pad_sequences(target_sequences, maxlen=target_maxlen, padding='post')\n","\n","# Split target sequences into input and output sequences\n","target_input_sequences = target_sequences[:, :-1]\n","target_output_sequences = target_sequences[:, 1:]\n","\n","# Define the Seq2Seq model with Attention\n","latent_dim = 256\n","\n","# Encoder\n","encoder_inputs = Input(shape=(input_maxlen,))\n","encoder_embedding = Embedding(input_vocab_size, latent_dim)(encoder_inputs)\n","encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n","encoder_states = [state_h, state_c]\n","\n","# Decoder\n","decoder_inputs = Input(shape=(None,))\n","decoder_embedding = Embedding(target_vocab_size, latent_dim)(decoder_inputs)\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n","\n","# Attention mechanism\n","attention = tf.keras.layers.Attention()\n","attention_output = attention([decoder_outputs, encoder_outputs])\n","decoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attention_output])\n","\n","# Dense layer to generate predictions\n","decoder_dense = TimeDistributed(Dense(target_vocab_size, activation='softmax'))\n","decoder_outputs = decoder_dense(decoder_concat_input)\n","\n","# Define the model\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n","\n","# Train the model\n","model.fit([input_sequences, target_input_sequences], target_output_sequences,\n","          batch_size=64, epochs=100, validation_split=0.2)\n","\n","# Inference models for translation\n","# Encoder model\n","encoder_model = Model(encoder_inputs, [encoder_outputs] + encoder_states)\n","\n","# Decoder model\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_hidden_state_input = Input(shape=(input_maxlen, latent_dim))\n","\n","decoder_outputs_inference, state_h, state_c = decoder_lstm(\n","    decoder_embedding, initial_state=decoder_states_inputs)\n","\n","attention_output_inference = attention([decoder_outputs_inference, decoder_hidden_state_input])\n","decoder_concat_input_inference = Concatenate(axis=-1)([decoder_outputs_inference, attention_output_inference])\n","decoder_outputs_inference = decoder_dense(decoder_concat_input_inference)\n","\n","decoder_model = Model(\n","    [decoder_inputs, decoder_hidden_state_input] + decoder_states_inputs,\n","    [decoder_outputs_inference, state_h, state_c])\n","\n","# Function to decode the sequence\n","def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    encoder_outputs, state_h, state_c = encoder_model.predict(input_seq)\n","    states_value = [state_h, state_c]\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1))\n","\n","    # Populate the first token of target sequence with the start token.\n","    # Assuming 'bonjour' is the start token.\n","    if 'bonjour' in target_tokenizer.word_index:\n","        target_seq[0, 0] = target_tokenizer.word_index['bonjour']\n","    else:\n","        print(\"Warning: 'bonjour' not found in target vocabulary. Using index 1 as a fallback.\")\n","        target_seq[0, 0] = 1 # Using 1 as a fallback, adjust if needed\n","\n","    # Sampling loop for a batch of sequences\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq, encoder_outputs] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        # Skip the padding token (index 0)\n","        if sampled_token_index == 0:\n","            break\n","        sampled_word = target_tokenizer.index_word[sampled_token_index]\n","        decoded_sentence += ' ' + sampled_word\n","\n","        # Exit condition: either hit max length or find stop token.\n","        if (sampled_word == '.' or len(decoded_sentence.split()) > target_maxlen):\n","            stop_condition = True\n","\n","        # Update the target sequence (length 1).\n","        target_seq = np.zeros((1, 1))\n","        target_seq[0, 0] = sampled_token_index\n","\n","        # Update states\n","        states_value = [h, c]\n","\n","    return decoded_sentence.strip() # Remove leading/trailing whitespace\n","\n","\n","# Test the model\n","for seq_index in range(5):\n","    input_seq = input_sequences[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-')\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)"],"metadata":{"id":"auzzpHJCy9HX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 9.3 Transformer Models"],"metadata":{"id":"f4jlDMwczDxH"}},{"cell_type":"markdown","source":["9.3.3 Implementing Transformer Models in TensorFlow"],"metadata":{"id":"qsBhQpt7zHhE"}},{"cell_type":"code","source":["from transformers import T5ForConditionalGeneration, T5Tokenizer\n","\n","# Load the pre-trained T5 model and tokenizer\n","model_name = \"t5-small\"\n","model = T5ForConditionalGeneration.from_pretrained(model_name)\n","tokenizer = T5Tokenizer.from_pretrained(model_name)\n","\n","# Sample text\n","text = \"\"\"translate English to French: Machine learning is a subset of artificial intelligence. It involves algorithms and statistical models to perform tasks without explicit instructions. Machine learning is widely used in various applications such as image recognition, natural language processing, and autonomous driving. It relies on patterns and inference instead of predefined rules.\"\"\"\n","\n","# Tokenize and encode the text\n","inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True)\n","\n","# Generate the translation\n","output_ids = model.generate(inputs, max_length=150, num_beams=4, early_stopping=True)\n","translation = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","\n","print(\"Translation:\")\n","print(translation)"],"metadata":{"id":"G04UnCGUzMEi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["9.3.4 Example: Visualizing Self-Attention Scores"],"metadata":{"id":"qtvc7yWlzNms"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","from transformers import T5ForConditionalGeneration, T5Tokenizer\n","\n","# Load the pre-trained T5 model and tokenizer\n","model_name = \"t5-small\"\n","model = T5ForConditionalGeneration.from_pretrained(model_name)\n","tokenizer = T5Tokenizer.from_pretrained(model_name)\n","\n","# Function to visualize attention scores\n","def visualize_attention(model, tokenizer, text):\n","    inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True)\n","    # Ensure the model is in evaluation mode if applicable (for PyTorch models)\n","    # model.eval()\n","\n","    outputs = model.generate(inputs, output_attentions=True, return_dict_in_generate=True)\n","\n","    # Access attentions correctly - typically in encoder_attentions or decoder_attentions\n","    # For T5, attentions are usually in the output dictionary\n","    if hasattr(outputs, 'encoder_attentions') and outputs.encoder_attentions is not None:\n","        attentions = outputs.encoder_attentions\n","        print(\"Using encoder attentions.\")\n","    elif hasattr(outputs, 'decoder_attentions') and outputs.decoder_attentions is not None:\n","        attentions = outputs.decoder_attentions\n","        print(\"Using decoder attentions.\")\n","    elif hasattr(outputs, 'cross_attentions') and outputs.cross_attentions is not None:\n","        attentions = outputs.cross_attentions\n","        print(\"Using cross attentions.\")\n","    else:\n","        print(\"Attention outputs not found in the model's generate output.\")\n","        return\n","\n","\n","    # Convert to numpy array for visualization\n","    # This assumes attentions is a tuple of tensors, and we want the first layer's attention for the first head\n","    # You might need to adjust this based on the model's output structure\n","    # Assuming attentions is a tuple of tensors, take the last layer and first head\n","    if isinstance(attentions, tuple) and len(attentions) > 0:\n","        attention_matrix = attentions[-1][0][0].detach().numpy()\n","    else:\n","        print(\"Attention outputs are not in the expected format.\")\n","        return\n","\n","\n","    # Plot the attention scores\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(attention_matrix, cmap=\"viridis\")\n","    plt.title(\"Self-Attention Scores\")\n","    plt.xlabel(\"Input Tokens\")\n","    plt.ylabel(\"Output Tokens\")\n","    plt.show()\n","\n","# Visualize attention scores for a sample sentence\n","sample_text = \"translate English to French: How are you?\"\n","\n","# Check if model and tokenizer are defined before visualizing\n","if 'model' in locals() and 'tokenizer' in locals():\n","    visualize_attention(model, tokenizer, sample_text)\n","else:\n","    print(\"Model and tokenizer are not defined. Please ensure they are loaded.\")"],"metadata":{"id":"1lplfkvszOKX"},"execution_count":null,"outputs":[]}]}
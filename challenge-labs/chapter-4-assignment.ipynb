{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyP1pt/GCEAjwBk8fJ9j1Mvx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 4.1 N-grams"],"metadata":{"id":"beUMXFwbcB2b"}},{"cell_type":"markdown","source":["4.1.2 Generating N-grams in Python"],"metadata":{"id":"Gd2eHGLGcGXj"}},{"cell_type":"code","source":["from nltk import ngrams\n","from collections import Counter\n","import nltk\n","nltk.download('punkt')\n","\n","# Sample text\n","text = \"Natural Language Processing is a fascinating field of study.\"\n","\n","# Tokenize the text into words\n","tokens = nltk.word_tokenize(text)\n","\n","# Function to generate N-grams\n","def generate_ngrams(tokens, n):\n","    n_grams = ngrams(tokens, n)\n","    return [' '.join(grams) for grams in n_grams]\n","\n","# Generate unigrams, bigrams, and trigrams\n","unigrams = generate_ngrams(tokens, 1)\n","bigrams = generate_ngrams(tokens, 2)\n","trigrams = generate_ngrams(tokens, 3)\n","\n","print(\"Unigrams:\")\n","print(unigrams)\n","print(\"\\\\nBigrams:\")\n","print(bigrams)\n","print(\"\\\\nTrigrams:\")\n","print(trigrams)"],"metadata":{"id":"6PFYkYoZcHok"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4.1.4 Training an N-gram Language Model"],"metadata":{"id":"xi967Y-NcJdz"}},{"cell_type":"code","source":["rom collections import defaultdict\n","\n","# Sample text corpus\n","corpus = [\n","    \"Natural Language Processing is a fascinating field of study.\",\n","    \"Machine learning and NLP are closely related.\",\n","    \"Language models are essential for NLP tasks.\"\n","]\n","\n","# Tokenize the text into words\n","tokenized_corpus = [nltk.word_tokenize(sentence) for sentence in corpus]\n","\n","# Function to calculate bigram probabilities\n","def train_bigram_model(tokenized_corpus):\n","    model = defaultdict(lambda: defaultdict(lambda: 0))\n","\n","    # Count bigrams\n","    for sentence in tokenized_corpus:\n","        for w1, w2 in ngrams(sentence, 2):\n","            model[w1][w2] += 1\n","\n","    # Calculate probabilities\n","    for w1 in model:\n","        total_count = float(sum(model[w1].values()))\n","        for w2 in model[w1]:\n","            model[w1][w2] /= total_count\n","\n","    return model\n","\n","# Train the bigram model\n","bigram_model = train_bigram_model(tokenized_corpus)\n","\n","# Function to get the probability of a bigram\n","def get_bigram_probability(bigram_model, w1, w2):\n","    return bigram_model[w1][w2]\n","\n","print(\"Bigram Probability (NLP | for):\")\n","print(get_bigram_probability(bigram_model, 'for', 'NLP'))"],"metadata":{"id":"8ovaojr4cNQC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4.2 Hidden Markov Models"],"metadata":{"id":"dH2NsaUxdSnt"}},{"cell_type":"markdown","source":["4.2.3 Implementing HMMs in Python"],"metadata":{"id":"gm9nKOZRdUFT"}},{"cell_type":"code","source":["!pip install hmmlearn"],"metadata":{"id":"aNdcFBaDdVmB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from hmmlearn import hmm\n","\n","# Define the states and observations\n","states = [\"Noun\", \"Verb\"]\n","n_states = len(states)\n","\n","observations = [\"I\", \"run\", \"to\", \"the\", \"store\"]\n","n_observations = len(observations)\n","\n","# Transition probability matrix (A)\n","transition_probability = np.array([\n","    [0.7, 0.3],  # From Noun to [Noun, Verb]\n","    [0.4, 0.6]   # From Verb to [Noun, Verb]\n","])\n","\n","# Emission probability matrix (B)\n","emission_probability = np.array([\n","    [0.2, 0.3, 0.2, 0.1, 0.2],  # From Noun to [\"I\", \"run\", \"to\", \"the\", \"store\"]\n","    [0.1, 0.6, 0.1, 0.1, 0.1]   # From Verb to [\"I\", \"run\", \"to\", \"the\", \"store\"]\n","])\n","\n","# Initial probability vector (pi)\n","start_probability = np.array([0.6, 0.4])  # [Noun, Verb]\n","\n","# Create the HMM model\n","model = hmm.MultinomialHMM(n_components=n_states)\n","model.startprob_ = start_probability\n","model.transmat_ = transition_probability\n","model.emissionprob_ = emission_probability\n","\n","# Encode the observations to integers\n","observation_sequence = [0, 1, 2, 3, 4]  # \"I\", \"run\", \"to\", \"the\", \"store\"\n","observation_sequence = np.array(observation_sequence).reshape(-1, 1)\n","\n","# Predict the hidden states (decoding problem)\n","logprob, hidden_states = model.decode(observation_sequence, algorithm=\"viterbi\")\n","\n","print(\"Observations:\", [observations[i] for i in observation_sequence.flatten()])\n","print(\"Hidden states:\", [states[i] for i in hidden_states])"],"metadata":{"id":"OytR-0X1dZyF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4.2.4 Solving the Three Fundamental Problems of HMMs"],"metadata":{"id":"9MyWTguAdflK"}},{"cell_type":"code","source":["# Sample data: sequences of observations\n","training_sequences = [\n","    [0, 1, 2, 3, 4],  # \"I run to the store\"\n","    [4, 2, 0, 1, 3],  # \"store to I run the\"\n","    [1, 2, 3, 0, 4],  # \"run to the I store\"\n","]\n","\n","# Convert the sequences to a format suitable for hmmlearn\n","training_sequences = [np.array(seq).reshape(-1, 1) for seq in training_sequences]\n","lengths = [len(seq) for seq in training_sequences]\n","training_data = np.concatenate(training_sequences)\n","\n","# Create and train the HMM model\n","model = hmm.MultinomialHMM(n_components=n_states, n_iter=100)\n","model.fit(training_data, lengths)\n","\n","print(\"Learned start probabilities:\")\n","print(model.startprob_)\n","\n","print(\"Learned transition probabilities:\")\n","print(model.transmat_)\n","\n","print(\"Learned emission probabilities:\")\n","print(model.emissionprob_)"],"metadata":{"id":"UQGXl9QRdg8_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4.3 Recurrent Neural Networks (RNNs)"],"metadata":{"id":"UEgFN47KdrnT"}},{"cell_type":"markdown","source":["4.3.3 Implementing RNNs in Python with TensorFlow/Keras\n"],"metadata":{"id":"sna9F4hBdnsq"}},{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"id":"jItlsPqndwru"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, SimpleRNN\n","from tensorflow.keras.utils import to_categorical\n","\n","# Sample text corpus\n","text = \"hello world\"\n","\n","# Create a character-level vocabulary\n","chars = sorted(set(text))\n","char_to_idx = {char: idx for idx, char in enumerate(chars)}\n","idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n","\n","# Create input-output pairs for training\n","sequence_length = 3\n","X = []\n","y = []\n","for i in range(len(text) - sequence_length):\n","    X.append([char_to_idx[char] for char in text[i:i + sequence_length]])\n","    y.append(char_to_idx[text[i + sequence_length]])\n","\n","X = np.array(X)\n","y = to_categorical(y, num_classes=len(chars))\n","\n","# Reshape input to be compatible with RNN input\n","X = X.reshape((X.shape[0], X.shape[1], 1))\n","\n","# Define the RNN model\n","model = Sequential()\n","model.add(SimpleRNN(50, input_shape=(sequence_length, 1)))\n","model.add(Dense(len(chars), activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy')\n","\n","# Train the model\n","model.fit(X, y, epochs=200, verbose=1)\n","\n","# Function to generate text using the trained model\n","def generate_text(model, start_string, num_generate):\n","    input_eval = [char_to_idx[s] for s in start_string]\n","    input_eval = np.array(input_eval).reshape((1, len(input_eval), 1))\n","\n","    text_generated = []\n","\n","    for i in range(num_generate):\n","        predictions = model.predict(input_eval)\n","        predicted_id = np.argmax(predictions[-1])\n","\n","        input_eval = np.append(input_eval[:, 1:], [[predicted_id]], axis=1)\n","        text_generated.append(idx_to_char[predicted_id])\n","\n","    return start_string + ''.join(text_generated)\n","\n","# Generate new text\n","start_string = \"hel\"\n","generated_text = generate_text(model, start_string, 5)\n","print(\"Generated text:\")\n","print(generated_text)"],"metadata":{"id":"B03MhxN2d1SP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4.3.4 Evaluating RNN Performance"],"metadata":{"id":"__Br-Y2Sd7XI"}},{"cell_type":"markdown","source":["4.3.5 Improving RNNs"],"metadata":{"id":"qurI14E3d9oM"}},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","\n","# Define the LSTM model\n","model = Sequential()\n","model.add(LSTM(50, input_shape=(sequence_length, num_features)))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy')\n","\n","# Train the model\n","model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))"],"metadata":{"id":"-f8ghqw9d_FG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","# 4.4 Long Short-Term Memory Networks (LSTMs)"],"metadata":{"id":"sawlFtfHeCnr"}},{"cell_type":"markdown","source":["4.4.2 Implementing LSTMs in Python with TensorFlow/Keras"],"metadata":{"id":"obJaTAjSeHgE"}},{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"id":"SU9mEgrdeQ3L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM\n","from tensorflow.keras.utils import to_categorical\n","\n","# Sample text corpus\n","text = \"hello world\"\n","\n","# Create a character-level vocabulary\n","chars = sorted(set(text))\n","char_to_idx = {char: idx for idx, char in enumerate(chars)}\n","idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n","\n","# Create input-output pairs for training\n","sequence_length = 3\n","X = []\n","y = []\n","for i in range(len(text) - sequence_length):\n","    X.append([char_to_idx[char] for char in text[i:i + sequence_length]])\n","    y.append(char_to_idx[text[i + sequence_length]])\n","\n","X = np.array(X)\n","y = to_categorical(y, num_classes=len(chars))\n","\n","# Reshape input to be compatible with LSTM input\n","X = X.reshape((X.shape[0], X.shape[1], 1))\n","\n","# Define the LSTM model\n","model = Sequential()\n","model.add(LSTM(50, input_shape=(sequence_length, 1)))\n","model.add(Dense(len(chars), activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy')\n","\n","# Train the model\n","model.fit(X, y, epochs=200, verbose=1)\n","\n","# Function to generate text using the trained model\n","def generate_text(model, start_string, num_generate):\n","    input_eval = [char_to_idx[s] for s in start_string]\n","    input_eval = np.array(input_eval).reshape((1, len(input_eval), 1))\n","\n","    text_generated = []\n","\n","    for i in range(num_generate):\n","        predictions = model.predict(input_eval)\n","        predicted_id = np.argmax(predictions[-1])\n","\n","        input_eval = np.append(input_eval[:, 1:], [[predicted_id]], axis=1)\n","        text_generated.append(idx_to_char[predicted_id])\n","\n","    return start_string + ''.join(text_generated)\n","\n","# Generate new text\n","start_string = \"hel\"\n","generated_text = generate_text(model, start_string, 5)\n","print(\"Generated text:\")\n","print(generated_text)"],"metadata":{"id":"NJuDvpVGeVQq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Chapter-4 Assignments"],"metadata":{"id":"onqM5GkyecGh"}},{"cell_type":"markdown","source":["Exercise 1: N-grams"],"metadata":{"id":"Rv6UNlkUeg58"}},{"cell_type":"code","source":["from nltk import ngrams\n","import nltk\n","nltk.download('punkt')\n","\n","# Sample text\n","text = \"Natural Language Processing with Python\"\n","\n","# Tokenize the text into words\n","tokens = nltk.word_tokenize(text)\n","\n","# Generate trigrams\n","trigrams = ngrams(tokens, 3)\n","\n","print(\"Trigrams:\")\n","for grams in trigrams:\n","    print(grams)"],"metadata":{"id":"aPtDaOeueivm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"EOIrBrABhA7N"}},{"cell_type":"markdown","source":["Exercise 2: Bigram Language Model"],"metadata":{"id":"euyam56Jellx"}},{"cell_type":"code","source":["corpus = [\n","    \"Natural Language Processing is fascinating.\",\n","    \"Language models are important in NLP.\",\n","    \"Machine learning and NLP are closely related.\"\n","]"],"metadata":{"id":"zNHvS8eIemJH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import defaultdict\n","import numpy as np\n","from nltk import ngrams\n","import nltk\n","nltk.download('punkt')\n","\n","# Sample text corpus\n","corpus = [\n","    \"Natural Language Processing is fascinating.\",\n","    \"Language models are important in NLP.\",\n","    \"Machine learning and NLP are closely related.\"\n","]\n","\n","# Tokenize the text into words\n","tokenized_corpus = [nltk.word_tokenize(sentence) for sentence in corpus]\n","\n","# Function to calculate bigram probabilities\n","def train_bigram_model(tokenized_corpus):\n","    model = defaultdict(lambda: defaultdict(lambda: 0))\n","\n","    # Count bigrams\n","    for sentence in tokenized_corpus:\n","        for w1, w2 in ngrams(sentence, 2):\n","            model[w1][w2] += 1\n","\n","    # Calculate probabilities\n","    for w1 in model:\n","        total_count = float(sum(model[w1].values()))\n","        for w2 in model[w1]:\n","            model[w1][w2] /= total_count\n","\n","    return model\n","\n","# Train the bigram model\n","bigram_model = train_bigram_model(tokenized_corpus)\n","\n","# Function to get the probability of a bigram\n","def get_bigram_probability(bigram_model, w1, w2):\n","    return bigram_model[w1][w2]\n","\n","print(\"Bigram Probability (Processing | Language):\")\n","print(get_bigram_probability(bigram_model, 'Language', 'Processing'))"],"metadata":{"id":"OKBuCdvneniy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"52hcANPFhCGq"}},{"cell_type":"markdown","source":["Exercise 3: HMM for Part-of-Speech Tagging"],"metadata":{"id":"WNZINlPzermh"}},{"cell_type":"code","source":["sentences = [\n","    [\"I\", \"run\", \"to\", \"the\", \"store\"],\n","    [\"She\", \"jumps\", \"over\", \"the\", \"fence\"]\n","]\n","\n","tags = [\n","    [\"PRON\", \"VERB\", \"ADP\", \"DET\", \"NOUN\"],\n","    [\"PRON\", \"VERB\", \"ADP\", \"DET\", \"NOUN\"]\n","]"],"metadata":{"id":"Q9NOgn86esOT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"nWo2b4LohC5s"}},{"cell_type":"code","source":["import numpy as np\n","from hmmlearn import hmm\n","\n","# Define the states and observations\n","states = [\"PRON\", \"VERB\", \"ADP\", \"DET\", \"NOUN\"]\n","n_states = len(states)\n","\n","observations = [\"I\", \"run\", \"to\", \"the\", \"store\", \"She\", \"jumps\", \"over\", \"fence\"]\n","n_observations = len(observations)\n","\n","# Encode the states and observations\n","state_to_idx = {state: idx for idx, state in enumerate(states)}\n","observation_to_idx = {obs: idx for idx, obs in enumerate(observations)}\n","\n","# Create the sequences for training\n","X = [[observation_to_idx[word] for word in sentence] for sentence in sentences]\n","y = [[state_to_idx[tag] for tag in tag_sequence] for tag_sequence in tags]\n","\n","# Convert to numpy arrays\n","X = np.concatenate([np.array(x).reshape(-1, 1) for x in X])\n","lengths = [len(x) for x in sentences]\n","y = np.concatenate(y)\n","\n","# Create the HMM model\n","model = hmm.MultinomialHMM(n_components=n_states, n_iter=100)\n","model.fit(X, lengths)\n","\n","# Predict the hidden states (decoding problem)\n","logprob, hidden_states = model.decode(X, algorithm=\"viterbi\")\n","\n","# Map the states back to their original labels\n","hidden_states = [states[state] for state in hidden_states]\n","\n","print(\"Observations:\", sentences[0] + sentences[1])\n","print(\"Predicted states:\", hidden_states)"],"metadata":{"id":"kzOHW6KfevYv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"DKklGJoxhD0I"}},{"cell_type":"markdown","source":["Exercise 4: Simple RNN for Text Generation"],"metadata":{"id":"RRQDOdIAexId"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, SimpleRNN\n","from tensorflow.keras.utils import to_categorical\n","\n","# Sample text corpus\n","text = \"hello world\"\n","\n","# Create a character-level vocabulary\n","chars = sorted(set(text))\n","char_to_idx = {char: idx for idx, char in enumerate(chars)}\n","idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n","\n","# Create input-output pairs for training\n","sequence_length = 3\n","X = []\n","y = []\n","for i in range(len(text) - sequence_length):\n","    X.append([char_to_idx[char] for char in text[i:i + sequence_length]])\n","    y.append(char_to_idx[text[i + sequence_length]])\n","\n","X = np.array(X)\n","y = to_categorical(y, num_classes=len(chars))\n","\n","# Reshape input to be compatible with RNN input\n","X = X.reshape((X.shape[0], X.shape[1], 1))\n","\n","# Define the RNN model\n","model = Sequential()\n","model.add(SimpleRNN(50, input_shape=(sequence_length, 1)))\n","model.add(Dense(len(chars), activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy')\n","\n","# Train the model\n","model.fit(X, y, epochs=200, verbose=1)\n","\n","# Function to generate text using the trained model\n","def generate_text(model, start_string, num_generate):\n","    input_eval = [char_to_idx[s] for s in start_string]\n","    input_eval = np.array(input_eval).reshape((1, len(input_eval), 1))\n","\n","    text_generated = []\n","\n","    for i in range(num_generate):\n","        predictions = model.predict(input_eval)\n","        predicted_id = np.argmax(predictions[-1])\n","\n","        input_eval = np.append(input_eval[:, 1:], [[predicted_id]], axis=1)\n","        text_generated.append(idx_to_char[predicted_id])\n","\n","    return start_string + ''.join(text_generated)\n","\n","# Generate new text\n","start_string = \"hel\"\n","generated_text = generate_text(model, start_string, 5)\n","print(\"Generated text:\")\n","print(generated_text)"],"metadata":{"id":"ALzs_etUexv-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"sMi3rv2ahEzm"}},{"cell_type":"markdown","source":["Exercise 5: LSTM for Text Generation"],"metadata":{"id":"cySCijdwe6Av"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM\n","from tensorflow.keras.utils import to_categorical\n","\n","# Sample text corpus\n","text = \"hello world\"\n","\n","# Create a character-level vocabulary\n","chars = sorted(set(text))\n","char_to_idx = {char: idx for idx, char in enumerate(chars)}\n","idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n","\n","# Create input-output pairs for training\n","sequence_length = 3\n","X = []\n","y = []\n","for i in range(len(text) - sequence_length):\n","    X.append([char_to_idx[char] for char in text[i:i + sequence_length]])\n","    y.append(char_to_idx[text[i + sequence_length]])\n","\n","X = np.array(X)\n","y = to_categorical(y, num_classes=len(chars))\n","\n","# Reshape input to be compatible with LSTM input\n","X = X.reshape((X.shape[0], X.shape[1], 1))\n","\n","# Define the LSTM model\n","model = Sequential()\n","model.add(LSTM(50, input_shape=(sequence_length, 1)))\n","model.add(Dense(len(chars), activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy')\n","\n","# Train the model\n","model.fit(X, y, epochs=200, verbose=1)\n","\n","# Function to generate text using the trained model\n","def generate_text(model, start_string, num_generate):\n","    input_eval = [char_to_idx[s] for s in start_string]\n","    input_eval = np.array(input_eval).reshape((1, len(input_eval), 1))\n","\n","    text_generated = []\n","\n","    for i in range(num_generate):\n","        predictions = model.predict(input_eval)\n","        predicted_id =\n","\n"," np.argmax(predictions[-1])\n","\n","        input_eval = np.append(input_eval[:, 1:], [[predicted_id]], axis=1)\n","        text_generated.append(idx_to_char[predicted_id])\n","\n","    return start_string + ''.join(text_generated)\n","\n","# Generate new text\n","start_string = \"hel\"\n","generated_text = generate_text(model, start_string, 5)\n","print(\"Generated text:\")\n","print(generated_text)"],"metadata":{"id":"2_EAxs4ye8lC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"j5UjFnbDhFsy"}}]}
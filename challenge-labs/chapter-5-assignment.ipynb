{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPLJSLndx97HzA5RtFrFJ0M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Chapter-5 Assignments"],"metadata":{"id":"xzBKvkZpgcYG"}},{"cell_type":"markdown","source":["**Installed the required Python prerequisite packages and libraries.**"],"metadata":{"id":"Dv3e3H0vBMtY"}},{"cell_type":"code","source":[],"metadata":{"id":"XiRpxC6zBObJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Exercise 1: Parts of Speech (POS) Tagging"],"metadata":{"id":"bWNfsFe2gmo3"}},{"cell_type":"code","source":["import nltk\n","from nltk import word_tokenize, pos_tag\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","nltk.download('averaged_perceptron_tagger_eng')\n","\n","# Sample text\n","text = \"The quick brown fox jumps over the lazy dog.\"\n","\n","# Tokenize the text into words\n","tokens = word_tokenize(text)\n","\n","# Perform POS tagging\n","pos_tags = pos_tag(tokens)\n","\n","print(\"POS Tags:\")\n","print(pos_tags)"],"metadata":{"id":"5RWaVIIMgpRG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"_tMV-wfxhIG2"}},{"cell_type":"markdown","source":["Exercise 2: Named Entity Recognition (NER)"],"metadata":{"id":"pCNUbGU6gq1m"}},{"cell_type":"code","source":["import spacy\n","\n","# Load the pre-trained spaCy model\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Sample text\n","text = \"Barack Obama was born on August 4, 1961, in Honolulu, Hawaii.\"\n","\n","# Process the text with the spaCy model\n","doc = nlp(text)\n","\n","# Print named entities with their labels\n","print(\"Named Entities:\")\n","for ent in doc.ents:\n","    print(ent.text, ent.label_)"],"metadata":{"id":"AUpTLZuugvh6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"BLNHn1p0hI57"}},{"cell_type":"markdown","source":["Exercise 3: Training a Custom NER Model"],"metadata":{"id":"f2KROyP9gxwy"}},{"cell_type":"code","source":["import spacy\n","from spacy.tokens import DocBin\n","from spacy.training import Example\n","from spacy.util import minibatch, compounding\n","\n","# Create a blank English model\n","nlp = spacy.blank(\"en\")\n","\n","# Create a new NER component and add it to the pipeline\n","ner = nlp.add_pipe(\"ner\")\n","\n","# Add labels to the NER component\n","ner.add_label(\"GADGET\")\n","\n","# Sample training data\n","TRAIN_DATA = [\n","    (\"Apple is releasing a new iPhone.\", {\"entities\": [(26, 32, \"GADGET\")]}),\n","    (\"The new iPad Pro is amazing.\", {\"entities\": [(8, 16, \"GADGET\")]}),\n","]\n","\n","# Convert the training data to spaCy's format\n","# Create a list of Example objects from the training data\n","examples = []\n","for text, annotations in TRAIN_DATA:\n","    doc = nlp.make_doc(text)\n","    example = Example.from_dict(doc, annotations)\n","    examples.append(example)\n","\n","\n","# Train the NER model\n","optimizer = nlp.begin_training()\n","for epoch in range(10):\n","    losses = {}\n","    batches = minibatch(examples, size=compounding(4.0, 32.0, 1.001))\n","    for batch in batches:\n","        # Pass the batch directly to nlp.update as it now contains Example objects\n","        nlp.update(batch, drop=0.5, losses=losses)\n","    print(\"Losses\", losses)\n","\n","# Test the trained model\n","doc = nlp(\"I just bought a new iPhone.\")\n","print(\"Named Entities:\", [(ent.text, ent.label_) for ent in doc.ents])"],"metadata":{"id":"O7rVcUlPgzZn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"i7tKi5y_hJuA"}},{"cell_type":"markdown","source":["Exercise 4: Dependency Parsing"],"metadata":{"id":"KhRl3sZyg24X"}},{"cell_type":"code","source":["import spacy\n","\n","# Load the pre-trained spaCy model\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Sample text\n","text = \"She enjoys reading books.\"\n","\n","# Process the text with the spaCy model\n","doc = nlp(text)\n","\n","# Print dependency parsing results\n","print(\"Dependency Parsing:\")\n","for token in doc:\n","    print(f\"{token.text} ({token.dep_}): {token.head.text}\")\n","\n","# Visualize the dependency tree (requires jupyter notebook or similar environment)\n","from spacy import displacy\n","displacy.render(doc, style=\"dep\", jupyter=True)"],"metadata":{"id":"js026ygzg3h5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"InnsROwKhKlZ"}},{"cell_type":"markdown","source":["Exercise 5: Training a Custom Dependency Parser"],"metadata":{"id":"TCkXCHt1g6SL"}},{"cell_type":"code","source":["from spacy.tokens import DocBin\n","from spacy.training import Example\n","from spacy.util import minibatch, compounding\n","\n","# Create a blank English model\n","nlp = spacy.blank(\"en\")\n","\n","# Create a new parser component and add it to the pipeline\n","parser = nlp.add_pipe(\"parser\")\n","\n","# Define labels for the parser\n","parser.add_label(\"nsubj\")\n","parser.add_label(\"dobj\")\n","parser.add_label(\"prep\")\n","parser.add_label(\"aux\") # Add aux label for 'playing' in the first sentence\n","parser.add_label(\"punct\") # Add punct label for '.'\n","\n","# Sample training data\n","# Corrected TRAIN_DATA to accurately reflect tokenization and dependencies\n","TRAIN_DATA = [\n","    (\"She enjoys playing tennis.\", {\"heads\": [1, 1, 1, 2, 1], \"deps\": [\"nsubj\", \"ROOT\", \"aux\", \"dobj\", \"punct\"]}),\n","    (\"I like reading books.\", {\"heads\": [1, 1, 2, 1], \"deps\": [\"nsubj\", \"ROOT\", \"dobj\", \"punct\"]}),\n","]\n","\n","# Convert the training data to spaCy's format\n","examples = []\n","for text, annotations in TRAIN_DATA:\n","    doc = nlp.make_doc(text)\n","    # Ensure annotations match the tokenization\n","    if len(doc) == len(annotations[\"heads\"]) and len(doc) == len(annotations[\"deps\"]):\n","        example = Example.from_dict(doc, annotations)\n","        examples.append(example)\n","    else:\n","        print(f\"Skipping example due to length mismatch: {text}\")\n","        print(f\"Doc length: {len(doc)}, Heads length: {len(annotations['heads'])}, Deps length: {len(annotations['deps'])}\")\n","\n","\n","# Train the parser\n","optimizer = nlp.begin_training()\n","for epoch in range(15): # Increased epochs for potentially better training\n","    losses = {}\n","    batches = minibatch(examples, size=compounding(4.0, 32.0, 1.001))\n","    for batch in batches:\n","        nlp.update(batch, drop=0.5, losses=losses)\n","    print(\"Losses\", losses)\n","\n","# Test the trained model\n","doc = nlp(\"She enjoys reading books.\")\n","for token in doc:\n","    print(f\"{token.text} ({token.dep_}): {token.head.text}\")"],"metadata":{"id":"F9Ttl18Cg6xA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"Xi1vDQe6hLWh"}}]}
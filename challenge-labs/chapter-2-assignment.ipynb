{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOEXK1Fih6GU3lrnoZQIryY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Chaper-2 Assignment"],"metadata":{"id":"PCEkQ54hWy0i"}},{"cell_type":"markdown","source":["Exercise 1: Stop Word Removal"],"metadata":{"id":"NOh1pnRXW4Or"}},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","# Sample text\n","text = \"NLP enables computers to understand human language, which is a crucial aspect of artificial intelligence.\"\n","\n","# Tokenize the text\n","tokens = text.split()\n","\n","# Remove stop words\n","stop_words = set(stopwords.words('english'))\n","filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n","\n","print(\"Original Tokens:\")\n","print(tokens)\n","\n","print(\"\\\\nFiltered Tokens:\")\n","print(filtered_tokens)"],"metadata":{"id":"04Lj-p6eW8zU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  \n"],"metadata":{"id":"ltujMc00XYE2"}},{"cell_type":"markdown","source":["Exercise 2: Stemming"],"metadata":{"id":"DWaFwtUHW9lL"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","\n","# Sample text\n","text = \"Stemming helps in reducing words to their root form, which can be beneficial for text processing.\"\n","\n","# Tokenize the text\n","tokens = text.split()\n","\n","# Initialize the stemmer\n","stemmer = PorterStemmer()\n","\n","# Stem the tokens\n","stemmed_tokens = [stemmer.stem(word) for word in tokens]\n","\n","print(\"Original Tokens:\")\n","print(tokens)\n","\n","print(\"\\\\nStemmed Tokens:\")\n","print(stemmed_tokens)"],"metadata":{"id":"yJoGXBgXW_IJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  \n"],"metadata":{"id":"qJe15Uh2XZog"}},{"cell_type":"markdown","source":["Exercise 3: Lemmatization"],"metadata":{"id":"LNw13BKUXCkl"}},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')\n","\n","# Sample text\n","text = \"Lemmatization is the process of reducing words to their base or root form.\"\n","\n","# Tokenize the text\n","tokens = text.split()\n","\n","# Initialize the lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","# Lemmatize the tokens\n","lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","print(\"Original Tokens:\")\n","print(tokens)\n","\n","print(\"\\\\nLemmatized Tokens:\")\n","print(lemmatized_tokens)"],"metadata":{"id":"2IF-gDRbXDTQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  \n"],"metadata":{"id":"XkukPWahXa95"}},{"cell_type":"markdown","source":["Exercise 4: Regular Expressions"],"metadata":{"id":"MG-lPHZ7XGCZ"}},{"cell_type":"code","source":["import re\n","\n","# Sample text\n","text = \"The project started on 2021-01-15 and ended on 2021-12-31.\"\n","\n","# Define a regex pattern to match dates in the format YYYY-MM-DD\n","pattern = r\"\\\\b\\\\d{4}-\\\\d{2}-\\\\d{2}\\\\b\"\n","\n","# Use re.findall() to find all matches\n","dates = re.findall(pattern, text)\n","\n","print(\"Extracted Dates:\")\n","print(dates)"],"metadata":{"id":"y9V6mLBOXG9p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  \n"],"metadata":{"id":"23uX5XjBXb0R"}},{"cell_type":"markdown","source":["Exercise 5: Word Tokenization"],"metadata":{"id":"jpec7idqXJZV"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","from nltk.tokenize import word_tokenize\n","\n","# Sample text\n","text = \"Tokenization is the first step in text preprocessing.\"\n","\n","# Perform word tokenization\n","tokens = word_tokenize(text)\n","\n","print(\"Word Tokens:\")\n","print(tokens)"],"metadata":{"id":"GIh0_oq9XKB9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  \n"],"metadata":{"id":"Bzvb2h2FXc29"}},{"cell_type":"markdown","source":["Exercise 6: Sentence Tokenization"],"metadata":{"id":"f1slKWL9XL-n"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","from nltk.tokenize import sent_tokenize\n","\n","# Sample text\n","text = \"Tokenization is essential. It breaks down text into smaller units.\"\n","\n","# Perform sentence tokenization\n","sentences = sent_tokenize(text)\n","\n","print(\"Sentences:\")\n","print(sentences)"],"metadata":{"id":"ZFE1oDvcXOBc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  \n"],"metadata":{"id":"VlsInQX3Xdth"}},{"cell_type":"markdown","source":["Exercise 7: Character Tokenization"],"metadata":{"id":"wIWQSlCKXQ3m"}},{"cell_type":"code","source":["def character_tokenization(text):\n","    # Perform character tokenization\n","    characters = list(text)\n","    return characters\n","\n","# Sample text\n","text = \"Character tokenization is useful for certain tasks.\"\n","\n","# Tokenize the text into characters\n","characters = character_tokenization(text)\n","\n","print(\"Character Tokens:\")\n","print(characters)"],"metadata":{"id":"cJzqV9tIXReC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  \n"],"metadata":{"id":"-OJZMSkeXebh"}}]}
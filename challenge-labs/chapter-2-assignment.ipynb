{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyM0OyWmITPhUAT/WgKwBu5B"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 2.1 Understanding Text Data"],"metadata":{"id":"coAy5rDTFEAA"}},{"cell_type":"markdown","source":["2.1.1 Nature of Text Data"],"metadata":{"id":"gUGsJcP-FIdB"}},{"cell_type":"code","source":["\"Natural Language Processing (NLP) enables computers to understand human language.\""],"metadata":{"id":"ip0A2aFAFKf8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.1.2 Importance of Text Preprocessing"],"metadata":{"id":"TGHtKTZuFXky"}},{"cell_type":"markdown","source":["2.1.3 Example: Exploring Raw Text Data"],"metadata":{"id":"ZIPt0DmcFact"}},{"cell_type":"code","source":["# Sample text\n","text = \"Natural Language Processing (NLP) enables computers to understand human language.\"\n","\n","# Display the text\n","print(\"Original Text:\")\n","print(text)\n","\n","# Length of the text\n","print(\"\\\\nLength of the text:\", len(text))\n","\n","# Unique characters in the text\n","unique_characters = set(text)\n","print(\"\\\\nUnique characters:\", unique_characters)\n","\n","# Number of words in the text\n","words = text.split()\n","print(\"\\\\nNumber of words:\", len(words))\n","\n","# Display the words\n","print(\"\\\\nWords in the text:\")\n","print(words)"],"metadata":{"id":"XyikXBjoFbSU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.1.4 Challenges with Text Data"],"metadata":{"id":"YdoOP8jUFnPG"}},{"cell_type":"markdown","source":["2.1.5 Practical Example: Basic Text Preprocessing Steps"],"metadata":{"id":"_Lsq3-dPFpAT"}},{"cell_type":"code","source":["import string\n","\n","# Sample text\n","text = \"Natural Language Processing (NLP) enables computers to understand human language.\"\n","\n","# Convert to lowercase\n","text = text.lower()\n","print(\"Lowercased Text:\")\n","print(text)\n","\n","# Remove punctuation\n","text = text.translate(str.maketrans('', '', string.punctuation))\n","print(\"\\\\nText without Punctuation:\")\n","print(text)\n","\n","# Tokenize the text\n","tokens = text.split()\n","print(\"\\\\nTokens:\")\n","print(tokens)"],"metadata":{"id":"9dzP-tYEFpxR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.2 Text Cleaning: Stop Word Removal, Stemming, Lemmatization"],"metadata":{"id":"RpA5OAJYFtjK"}},{"cell_type":"markdown","source":["2.2.1 Stop Word Removal"],"metadata":{"id":"RD8Fm-SyF0O6"}},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","# Sample text\n","text = \"Natural Language Processing enables computers to understand human language.\"\n","\n","# Tokenize the text\n","tokens = text.split()\n","\n","# Remove stop words\n","stop_words = set(stopwords.words('english'))\n","filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n","\n","print(\"Original Tokens:\")\n","print(tokens)\n","\n","print(\"\\\\nFiltered Tokens:\")\n","print(filtered_tokens)"],"metadata":{"id":"k2SV_zAAF1Ly"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.2.2 Stemming"],"metadata":{"id":"6JpagWfoF4IA"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","\n","# Sample text\n","text = \"Natural Language Processing enables computers to understand human language.\"\n","\n","# Tokenize the text\n","tokens = text.split()\n","\n","# Initialize the stemmer\n","stemmer = PorterStemmer()\n","\n","# Stem the tokens\n","stemmed_tokens = [stemmer.stem(word) for word in tokens]\n","\n","print(\"Original Tokens:\")\n","print(tokens)\n","\n","print(\"\\\\nStemmed Tokens:\")\n","print(stemmed_tokens)"],"metadata":{"id":"q8R8o5DiF6oL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.2.3 Lemmatization"],"metadata":{"id":"fpXXB5KbF8M2"}},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","# Sample text\n","text = \"Natural Language Processing enables computers to understand human language.\"\n","\n","# Tokenize the text\n","tokens = text.split()\n","\n","# Initialize the lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","# Lemmatize the tokens\n","lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","print(\"Original Tokens:\")\n","print(tokens)\n","\n","print(\"\\\\nLemmatized Tokens:\")\n","print(lemmatized_tokens)"],"metadata":{"id":"f5iQ2PJ9F91T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.2.4 Practical Example: Combining Text Cleaning Techniques"],"metadata":{"id":"l96kJJI9GAKm"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Sample text\n","text = \"Natural Language Processing enables computers to understand human language.\"\n","\n","# Convert to lowercase\n","text = text.lower()\n","\n","# Remove punctuation\n","import string\n","text = text.translate(str.maketrans('', '', string.punctuation))\n","\n","# Tokenize the text\n","tokens = text.split()\n","\n","# Remove stop words\n","stop_words = set(stopwords.words('english'))\n","filtered_tokens = [word for word in tokens if word not in stop_words]\n","\n","# Initialize the stemmer and lemmatizer\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","\n","# Stem and lemmatize the filtered tokens\n","processed_tokens = [lemmatizer.lemmatize(stemmer.stem(word)) for word in filtered_tokens]\n","\n","print(\"Original Text:\")\n","print(text)\n","\n","print(\"\\\\nFiltered Tokens (Stop Words Removed):\")\n","print(filtered_tokens)\n","\n","print(\"\\\\nProcessed Tokens (Stemmed and Lemmatized):\")\n","print(processed_tokens)"],"metadata":{"id":"BeoA8u9iGDdq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.3 Regular Expressions"],"metadata":{"id":"f632K_sXGONh"}},{"cell_type":"markdown","source":["2.3.1 Basics of Regular Expressions"],"metadata":{"id":"mSte4bHTGSDv"}},{"cell_type":"code","source":["import re\n","\n","# Sample text\n","text = \"The quick brown fox jumps over the lazy dog.\"\n","\n","# Define a pattern to search for the word \"fox\"\n","pattern = r\"fox\"\n","\n","# Use re.search() to find the pattern in the text\n","match = re.search(pattern, text)\n","\n","# Display the match\n","if match:\n","    print(\"Match found:\", match.group())\n","else:\n","    print(\"No match found.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rwCElGAjGRvr","executionInfo":{"status":"ok","timestamp":1757385413653,"user_tz":300,"elapsed":12,"user":{"displayName":"Jimmy Ardiansyah","userId":"06913112371375508078"}},"outputId":"0e137daf-4712-4a9b-fb32-584a80313022"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Match found: fox\n"]}]},{"cell_type":"markdown","source":["2.3.2 Common Regex Patterns and Syntax"],"metadata":{"id":"DZIomfyOGV69"}},{"cell_type":"markdown","source":["2.3.3 Practical Examples of Regex in Python"],"metadata":{"id":"xrFt-hC9GWia"}},{"cell_type":"code","source":["import re\n","\n","# Sample text with email addresses\n","text = \"Please contact us at support@example.com or sales@example.com for further information.\"\n","\n","# Define a regex pattern to match email addresses\n","pattern = r\"\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,}\\\\b\"\n","\n","# Use re.findall() to find all matches\n","emails = re.findall(pattern, text)\n","\n","# Display the extracted email addresses\n","print(\"Extracted Email Addresses:\")\n","print(emails)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S44C9VY5GZHO","executionInfo":{"status":"ok","timestamp":1757385512650,"user_tz":300,"elapsed":7,"user":{"displayName":"Jimmy Ardiansyah","userId":"06913112371375508078"}},"outputId":"a3dae6e7-8e62-4347-a62e-e5af184d04e5"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracted Email Addresses:\n","[]\n"]}]},{"cell_type":"code","source":["import re\n","\n","# Sample text with phone numbers\n","text = \"Contact us at (123) 456-7890 or (987) 654-3210.\"\n","\n","# Define a regex pattern to match phone numbers\n","pattern = r\"\\\\(\\\\d{3}\\\\) \\\\d{3}-\\\\d{4}\"\n","\n","# Use re.findall() to find all matches\n","phone_numbers = re.findall(pattern, text)\n","\n","# Display the extracted phone numbers\n","print(\"Extracted Phone Numbers:\")\n","print(phone_numbers)"],"metadata":{"id":"Lj_uxeffVrln"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","# Sample text\n","text = \"The quick brown fox jumps over the lazy dog. The fox is clever.\"\n","\n","# Define a pattern to match the word \"fox\"\n","pattern = r\"fox\"\n","\n","# Use re.sub() to replace \"fox\" with \"cat\"\n","new_text = re.sub(pattern, \"cat\", text)\n","\n","# Display the modified text\n","print(\"Modified Text:\")\n","print(new_text)"],"metadata":{"id":"Vz5mlv2dGg9S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.3.4 Advanced Regex Techniques"],"metadata":{"id":"iklHSOb0GjDv"}},{"cell_type":"code","source":["import re\n","\n","# Sample text with dates\n","text = \"The event is scheduled for 2022-08-15. Another event is on 15/08/2022.\"\n","\n","# Define a regex pattern to match dates\n","pattern = r\"\\\\b(?:\\\\d{4}-\\\\d{2}-\\\\d{2}|\\\\d{2}/\\\\d{2}/\\\\d{4})\\\\b\"\n","\n","# Use re.findall() to find all matches\n","dates = re.findall(pattern, text)\n","\n","# Display the extracted dates\n","print(\"Extracted Dates:\")\n","print(dates)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LBarilVcGkzv","executionInfo":{"status":"ok","timestamp":1757381628774,"user_tz":300,"elapsed":10,"user":{"displayName":"Jimmy Ardiansyah","userId":"06913112371375508078"}},"outputId":"7dfdc39b-0168-4058-c519-3a50a789f427"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracted Dates:\n","[]\n"]}]},{"cell_type":"code","source":["import re\n","\n","# Sample text with hashtags\n","text = \"Loving the new features of this product! #excited #newrelease #tech\"\n","\n","# Define a regex pattern to match hashtags\n","pattern = r\"#\\\\w+\"\n","\n","# Use re.findall() to find all matches\n","hashtags = re.findall(pattern, text)\n","\n","# Display the extracted hashtags\n","print(\"Extracted Hashtags:\")\n","print(hashtags)"],"metadata":{"id":"kJa8wC1QGmlB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.4 Tokenization"],"metadata":{"id":"YiBqrjuDV_ro"}},{"cell_type":"markdown","source":["2.4.3 Word Tokenization"],"metadata":{"id":"aQHka1EOWGJd"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('punkt_tab') # Add this line to download the missing resource\n","from nltk.tokenize import word_tokenize\n","\n","# Sample text\n","text = \"Natural Language Processing enables computers to understand human language.\"\n","\n","# Perform word tokenization\n","tokens = word_tokenize(text)\n","\n","print(\"Word Tokens:\")\n","print(tokens)"],"metadata":{"id":"mTJxZwaHWHE5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","\n","# Load SpaCy model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Sample text\n","text = \"Natural Language Processing enables computers to understand human language.\"\n","\n","# Perform word tokenization\n","doc = nlp(text)\n","tokens = [token.text for token in doc]\n","\n","print(\"Word Tokens:\")\n","print(tokens)"],"metadata":{"id":"Ag_3FOX6WKcw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.4.4 Sentence Tokenization"],"metadata":{"id":"6aos956fWdTI"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize\n","\n","# Sample text\n","text = \"Natural Language Processing enables computers to understand human language. It is a fascinating field.\"\n","\n","# Perform sentence tokenization\n","sentences = sent_tokenize(text)\n","\n","print(\"Sentences:\")\n","print(sentences)"],"metadata":{"id":"OJeLSkKTWfW1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","\n","# Load SpaCy model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Sample text\n","text = \"Natural Language Processing enables computers to understand human language. It is a fascinating field.\"\n","\n","# Perform sentence tokenization\n","doc = nlp(text)\n","sentences = [sent.text for sent in doc.sents]\n","\n","print(\"Sentences:\")\n","print(sentences)"],"metadata":{"id":"q5JuLnP6WjR-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.4.5 Character Tokenization"],"metadata":{"id":"I6yLWSqbWkhb"}},{"cell_type":"code","source":["# Sample text\n","text = \"Natural Language Processing\"\n","\n","# Perform character tokenization\n","characters = list(text)\n","\n","print(\"Characters:\")\n","print(characters)"],"metadata":{"id":"BvTMg3gFWmSa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.4.6 Practical Example: Tokenization Pipeline"],"metadata":{"id":"C8Uzpg_3Wr73"}},{"cell_type":"code","source":["import nltk\n","import spacy\n","nltk.download('punkt')\n","\n","# Load SpaCy model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Sample text\n","text = \"Natural Language Processing enables computers to understand human language. It is a fascinating field.\"\n","\n","# Perform word tokenization using NLTK\n","word_tokens = nltk.word_tokenize(text)\n","print(\"Word Tokens:\")\n","print(word_tokens)\n","\n","# Perform sentence tokenization using NLTK\n","sentence_tokens = nltk.sent_tokenize(text)\n","print(\"\\\\nSentence Tokens:\")\n","print(sentence_tokens)\n","\n","# Perform sentence tokenization using SpaCy\n","doc = nlp(text)\n","spacy_sentence_tokens = [sent.text for sent in doc.sents]\n","print(\"\\\\nSentence Tokens (SpaCy):\")\n","print(spacy_sentence_tokens)\n","\n","# Perform word tokenization using SpaCy\n","spacy_word_tokens = [token.text for token in doc]\n","print(\"\\\\nWord Tokens (SpaCy):\")\n","print(spacy_word_tokens)\n","\n","# Perform character tokenization\n","char_tokens = list(text)\n","print(\"\\\\nCharacter Tokens:\")\n","print(char_tokens)"],"metadata":{"id":"9aEBa3eZWpnq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Chaper-2 Assignment"],"metadata":{"id":"PCEkQ54hWy0i"}},{"cell_type":"markdown","source":["Exercise 1: Stop Word Removal"],"metadata":{"id":"NOh1pnRXW4Or"}},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","# Sample text\n","text = \"NLP enables computers to understand human language, which is a crucial aspect of artificial intelligence.\"\n","\n","# Tokenize the text\n","tokens = text.split()\n","\n","# Remove stop words\n","stop_words = set(stopwords.words('english'))\n","filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n","\n","print(\"Original Tokens:\")\n","print(tokens)\n","\n","print(\"\\\\nFiltered Tokens:\")\n","print(filtered_tokens)"],"metadata":{"id":"04Lj-p6eW8zU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  \n"],"metadata":{"id":"ltujMc00XYE2"}},{"cell_type":"markdown","source":["Exercise 2: Stemming"],"metadata":{"id":"DWaFwtUHW9lL"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","\n","# Sample text\n","text = \"Stemming helps in reducing words to their root form, which can be beneficial for text processing.\"\n","\n","# Tokenize the text\n","tokens = text.split()\n","\n","# Initialize the stemmer\n","stemmer = PorterStemmer()\n","\n","# Stem the tokens\n","stemmed_tokens = [stemmer.stem(word) for word in tokens]\n","\n","print(\"Original Tokens:\")\n","print(tokens)\n","\n","print(\"\\\\nStemmed Tokens:\")\n","print(stemmed_tokens)"],"metadata":{"id":"yJoGXBgXW_IJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  \n"],"metadata":{"id":"qJe15Uh2XZog"}},{"cell_type":"markdown","source":["Exercise 3: Lemmatization"],"metadata":{"id":"LNw13BKUXCkl"}},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')\n","\n","# Sample text\n","text = \"Lemmatization is the process of reducing words to their base or root form.\"\n","\n","# Tokenize the text\n","tokens = text.split()\n","\n","# Initialize the lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","# Lemmatize the tokens\n","lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","print(\"Original Tokens:\")\n","print(tokens)\n","\n","print(\"\\\\nLemmatized Tokens:\")\n","print(lemmatized_tokens)"],"metadata":{"id":"2IF-gDRbXDTQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  \n"],"metadata":{"id":"XkukPWahXa95"}},{"cell_type":"markdown","source":["Exercise 4: Regular Expressions"],"metadata":{"id":"MG-lPHZ7XGCZ"}},{"cell_type":"code","source":["import re\n","\n","# Sample text\n","text = \"The project started on 2021-01-15 and ended on 2021-12-31.\"\n","\n","# Define a regex pattern to match dates in the format YYYY-MM-DD\n","pattern = r\"\\\\b\\\\d{4}-\\\\d{2}-\\\\d{2}\\\\b\"\n","\n","# Use re.findall() to find all matches\n","dates = re.findall(pattern, text)\n","\n","print(\"Extracted Dates:\")\n","print(dates)"],"metadata":{"id":"y9V6mLBOXG9p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  \n"],"metadata":{"id":"23uX5XjBXb0R"}},{"cell_type":"markdown","source":["Exercise 5: Word Tokenization"],"metadata":{"id":"jpec7idqXJZV"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","\n","# Sample text\n","text = \"Tokenization is the first step in text preprocessing.\"\n","\n","# Perform word tokenization\n","tokens = word_tokenize(text)\n","\n","print(\"Word Tokens:\")\n","print(tokens)"],"metadata":{"id":"GIh0_oq9XKB9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  \n"],"metadata":{"id":"Bzvb2h2FXc29"}},{"cell_type":"markdown","source":["Exercise 6: Sentence Tokenization"],"metadata":{"id":"f1slKWL9XL-n"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize\n","\n","# Sample text\n","text = \"Tokenization is essential. It breaks down text into smaller units.\"\n","\n","# Perform sentence tokenization\n","sentences = sent_tokenize(text)\n","\n","print(\"Sentences:\")\n","print(sentences)"],"metadata":{"id":"ZFE1oDvcXOBc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  \n"],"metadata":{"id":"VlsInQX3Xdth"}},{"cell_type":"markdown","source":["Exercise 7: Character Tokenization"],"metadata":{"id":"wIWQSlCKXQ3m"}},{"cell_type":"code","source":["def character_tokenization(text):\n","    # Perform character tokenization\n","    characters = list(text)\n","    return characters\n","\n","# Sample text\n","text = \"Character tokenization is useful for certain tasks.\"\n","\n","# Tokenize the text into characters\n","characters = character_tokenization(text)\n","\n","print(\"Character Tokens:\")\n","print(characters)"],"metadata":{"id":"cJzqV9tIXReC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  \n"],"metadata":{"id":"-OJZMSkeXebh"}}]}
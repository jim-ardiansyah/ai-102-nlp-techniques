{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMW2dfhTBCsakANfkAiHGV2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 6.1 Rule-Based Approaches"],"metadata":{"id":"sA0gTW9rhVSw"}},{"cell_type":"markdown","source":["6.1.2 Implementing Rule-Based Sentiment Analysis"],"metadata":{"id":"to-kMIgqhXlO"}},{"cell_type":"code","source":["!pip install textblob"],"metadata":{"id":"rdau6IjBhZJs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from textblob import TextBlob\n","\n","# Sample text\n","text = \"I love this product! It works wonderfully and the quality is excellent.\"\n","\n","# Perform sentiment analysis\n","blob = TextBlob(text)\n","sentiment = blob.sentiment\n","\n","print(\"Sentiment Analysis:\")\n","print(f\"Polarity: {sentiment.polarity}, Subjectivity: {sentiment.subjectivity}\")"],"metadata":{"id":"pjPFL6mhhbDM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6.1.3 Creating Custom Rule-Based Sentiment Analyzers"],"metadata":{"id":"hdY451Eehd99"}},{"cell_type":"code","source":["!pip install afinn"],"metadata":{"id":"2CvHIvxXhei1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from afinn import Afinn\n","\n","# Initialize the Afinn sentiment analyzer\n","afinn = Afinn()\n","\n","# Sample text\n","text = \"I hate the traffic in this city. It makes commuting a nightmare.\"\n","\n","# Perform sentiment analysis\n","sentiment_score = afinn.score(text)\n","\n","# Determine sentiment based on score\n","if sentiment_score > 0:\n","    sentiment = \"Positive\"\n","elif sentiment_score < 0:\n","    sentiment = \"Negative\"\n","else:\n","    sentiment = \"Neutral\"\n","\n","print(\"Sentiment Analysis:\")\n","print(f\"Text: {text}\")\n","print(f\"Sentiment Score: {sentiment_score}\")\n","print(f\"Sentiment: {sentiment}\")"],"metadata":{"id":"TPxpjlgVhjwB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6.2 Machine Learning Approaches"],"metadata":{"id":"78GxBJjWhosH"}},{"cell_type":"markdown","source":["6.2.2 Feature Extraction"],"metadata":{"id":"YPADIZwFhuT-"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Sample text corpus\n","corpus = [\n","    \"I love this product! It's amazing.\",\n","    \"This is the worst service I have ever experienced.\",\n","    \"I am very happy with my purchase.\",\n","    \"I am disappointed with the quality of this item.\"\n","]\n","\n","# Initialize the TF-IDF Vectorizer\n","vectorizer = TfidfVectorizer()\n","\n","# Transform the text data into TF-IDF features\n","X = vectorizer.fit_transform(corpus)\n","\n","print(\"TF-IDF Feature Matrix:\")\n","print(X.toarray())"],"metadata":{"id":"GISLGGuthvi1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6.2.3 Model Training"],"metadata":{"id":"4cgQ7GX7hxRD"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Sample text corpus and labels\n","corpus = [\n","    \"I love this product! It's amazing.\",\n","    \"This is the worst service I have ever experienced.\",\n","    \"I am very happy with my purchase.\",\n","    \"I am disappointed with the quality of this item.\"\n","]\n","labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n","\n","# Transform the text data into TF-IDF features\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(corpus)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=42)\n","\n","# Initialize and train the Logistic Regression model\n","model = LogisticRegression()\n","model.fit(X_train, y_train)\n","\n","# Predict the sentiment of the test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","print(f\"Accuracy: {accuracy}\")\n","print(\"Classification Report:\")\n","print(report)"],"metadata":{"id":"FNV4cGh8hyc5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6.2.4 Evaluating Machine Learning Models"],"metadata":{"id":"cZzQ6cTgh1rq"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Predict the sentiment of the test set\n","y_pred = model.predict(X_test)\n","\n","# Calculate evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(f\"Accuracy: {accuracy}\")\n","print(f\"Precision: {precision}\")\n","print(f\"Recall: {recall}\")\n","print(f\"F1 Score: {f1}\")"],"metadata":{"id":"yV_D-bDjh2Sa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6.3 Deep Learning Approaches"],"metadata":{"id":"WklxblQBh4J8"}},{"cell_type":"markdown","source":["6.3.2 Convolutional Neural Networks (CNNs)"],"metadata":{"id":"DIgRhCJRh77I"}},{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"id":"x6lywXR7h9hy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Embedding\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","# Sample text corpus and labels\n","corpus = [\n","    \"I love this product! It's amazing.\",\n","    \"This is the worst service I have ever experienced.\",\n","    \"I am very happy with my purchase.\",\n","    \"I am disappointed with the quality of this item.\"\n","]\n","labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n","\n","# Tokenize and pad the text data\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(corpus)\n","X = tokenizer.texts_to_sequences(corpus)\n","X = pad_sequences(X, maxlen=10)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=42)\n","\n","# Define the CNN model\n","model = Sequential()\n","model.add(Embedding(input_dim=5000, output_dim=50, input_length=10))\n","model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n","model.add(GlobalMaxPooling1D())\n","model.add(Dense(10, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train, y_train, epochs=5, verbose=1, validation_data=(X_test, y_test))\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(f\"Accuracy: {accuracy}\")\n","\n","# Predict the sentiment of new text\n","new_text = [\"The product is excellent and I love it.\"]\n","new_text_seq = tokenizer.texts_to_sequences(new_text)\n","new_text_padded = pad_sequences(new_text_seq, maxlen=10)\n","prediction = model.predict(new_text_padded)\n","print(\"Prediction:\", \"Positive\" if prediction[0][0] > 0.5 else \"Negative\")"],"metadata":{"id":"SZItF2rSiGup"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6.3.3 Recurrent Neural Networks (RNNs) and Long Short-Term Memory Networks (LSTMs)\n"],"metadata":{"id":"QH4rJXU_iLpF"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM, Embedding\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","# Sample text corpus and labels\n","corpus = [\n","    \"I love this product! It's amazing.\",\n","    \"This is the worst service I have ever experienced.\",\n","    \"I am very happy with my purchase.\",\n","    \"I am disappointed with the quality of this item.\"\n","]\n","labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n","\n","# Tokenize and pad the text data\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(corpus)\n","X = tokenizer.texts_to_sequences(corpus)\n","X = pad_sequences(X, maxlen=10)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=42)\n","\n","# Define the LSTM model\n","model = Sequential()\n","model.add(Embedding(input_dim=5000, output_dim=50, input_length=10))\n","model.add(LSTM(100))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train, y_train, epochs=5, verbose=1, validation_data=(X_test, y_test))\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(f\"Accuracy: {accuracy}\")\n","\n","# Predict the sentiment of new text\n","new_text = [\"The product is excellent and I love it.\"]\n","new_text_seq = tokenizer.texts_to_sequences(new_text)\n","new_text_padded = pad_sequences(new_text_seq, maxlen=10)\n","prediction = model.predict(new_text_padded)\n","print(\"Prediction:\", \"Positive\" if prediction[0][0] > 0.5 else \"Negative\")"],"metadata":{"id":"vHgJ3vjGiMjh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6.3.4 Transformer-Based Models"],"metadata":{"id":"SLFtva59iQD1"}},{"cell_type":"code","source":["!pip install Transforms"],"metadata":{"id":"A7NZdacBiQgp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from transformers import BertTokenizer, TFBertForSequenceClassification\n","from sklearn.model_selection import train_test_split\n","\n","# Sample text corpus and labels\n","corpus = [\n","    \"I love this product! It's amazing.\",\n","    \"This is the worst service I have ever experienced.\",\n","    \"I am very happy with my purchase.\",\n","    \"I am disappointed with the quality of this item.\"\n","]\n","labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n","\n","# Initialize the BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenize and encode the text data\n","X = tokenizer(corpus, padding=True, truncation=True, max_length=10, return_tensors='tf')\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X['input_ids'], labels, test_size=0.25, random_state=42)\n","\n","# Initialize the BERT model for sequence classification\n","model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","\n","# Compile the model\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","\n"," metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train, np.array(y_train), epochs=3, batch_size=8, validation_data=(X_test, np.array(y_test)))\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X_test, np.array(y_test))\n","print(f\"Accuracy: {accuracy}\")\n","\n","# Predict the sentiment of new text\n","new_text = [\"The product is excellent and I love it.\"]\n","new_text_enc = tokenizer(new_text, padding=True, truncation=True, max_length=10, return_tensors='tf')\n","prediction = model.predict(new_text_enc['input_ids'])\n","print(\"Prediction:\", \"Positive\" if np.argmax(prediction.logits) == 1 else \"Negative\")"],"metadata":{"id":"vVrvJ6cCiVeR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Chapter-6 Assignments"],"metadata":{"id":"4pM0S3O8ialT"}},{"cell_type":"markdown","source":["\n","Exercise 1: Rule-Based Sentiment Analysis with TextBlob"],"metadata":{"id":"6k0KADm1ieWN"}},{"cell_type":"code","source":["from textblob import TextBlob\n","\n","# Sample texts\n","texts = [\n","    \"The weather is terrible today.\",\n","    \"I am so excited about the new movie release.\"\n","]\n","\n","# Perform sentiment analysis\n","for text in texts:\n","    blob = TextBlob(text)\n","    sentiment = blob.sentiment\n","    print(f\"Text: {text}\")\n","    print(f\"Polarity: {sentiment.polarity}, Subjectivity: {sentiment.subjectivity}\")\n","    print()"],"metadata":{"id":"sOdH15QWifyl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"Lnmt5l_6i0Zy"}},{"cell_type":"markdown","source":["Exercise 2: Custom Rule-Based Sentiment Analysis with Afinn"],"metadata":{"id":"IA3dLo1kihVb"}},{"cell_type":"code","source":["from afinn import Afinn\n","\n","# Initialize the Afinn sentiment analyzer\n","afinn = Afinn()\n","\n","# Sample texts\n","texts = [\n","    \"I hate waiting in long lines.\",\n","    \"The food at the restaurant was fantastic.\"\n","]\n","\n","# Perform sentiment analysis\n","for text in texts:\n","    sentiment_score = afinn.score(text)\n","    sentiment = \"Positive\" if sentiment_score > 0 else \"Negative\" if sentiment_score < 0 else \"Neutral\"\n","    print(f\"Text: {text}\")\n","    print(f\"Sentiment Score: {sentiment_score}\")\n","    print(f\"Sentiment: {sentiment}\")\n","    print()"],"metadata":{"id":"yo88M7xKiixm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"1RuubZ_ui1KP"}},{"cell_type":"markdown","source":["Exercise 3: Sentiment Analysis with Logistic Regression"],"metadata":{"id":"uayoS-DwimRD"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Sample text corpus and labels\n","corpus = [\n","    \"I love this product!\",\n","    \"This is the worst service.\",\n","    \"I am happy with my purchase.\",\n","    \"The quality is terrible.\"\n","]\n","labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n","\n","# Transform the text data into TF-IDF features\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(corpus)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=42)\n","\n","# Initialize and train the Logistic Regression model\n","model = LogisticRegression()\n","model.fit(X_train, y_train)\n","\n","# Predict the sentiment of the test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","print(f\"Accuracy: {accuracy}\")\n","print(\"Classification Report:\")\n","print(report)"],"metadata":{"id":"LS3LVOMmim2-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"AULtgIDDi2Be"}},{"cell_type":"markdown","source":["Exercise 4: Sentiment Analysis with LSTMs"],"metadata":{"id":"PDc5Y7Knio7j"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM, Embedding\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","# Sample text corpus and labels\n","corpus = [\n","    \"I love this product!\",\n","    \"This is the worst service.\",\n","    \"I am happy with my purchase.\",\n","    \"The quality is terrible.\"\n","]\n","labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n","\n","# Tokenize and pad the text data\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(corpus)\n","X = tokenizer.texts_to_sequences(corpus)\n","X = pad_sequences(X, maxlen=10)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=42)\n","\n","# Define the LSTM model\n","model = Sequential()\n","model.add(Embedding(input_dim=5000, output_dim=50, input_length=10))\n","model.add(LSTM(100))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train, y_train, epochs=5, verbose=1, validation_data=(X_test, y_test))\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(f\"Accuracy: {accuracy}\")\n","\n","# Predict the sentiment of new text\n","new_text = [\"The product is excellent and I love it.\"]\n","new_text_seq = tokenizer.texts_to_sequences(new_text)\n","new_text_padded = pad_sequences(new_text_seq, maxlen=10)\n","prediction = model.predict(new_text_padded)\n","print(\"Prediction:\", \"Positive\" if prediction[0][0] > 0.5 else \"Negative\")"],"metadata":{"id":"Lki6fCq9isWk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"p5xEH6Sqi3Jy"}},{"cell_type":"markdown","source":["Exercise 5: Sentiment Analysis with BERT"],"metadata":{"id":"wEtk2n8yit6y"}},{"cell_type":"code","source":["!pip install -U \"transformers>=4.43\" \"safetensors>=0.4.2\""],"metadata":{"id":"kT4E2lLnlkC8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from transformers import BertTokenizer, TFBertForSequenceClassification\n","from sklearn.model_selection import train_test_split\n","\n","# Sample text corpus and labels\n","corpus = [\n","    \"I love this product!\",\n","    \"This is the worst service.\",\n","    \"I am happy with my purchase.\",\n","    \"The quality is terrible.\"\n","]\n","labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n","\n","# Initialize the BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenize and encode the text data\n","X = tokenizer(corpus, padding=True, truncation=True, max_length=10, return_tensors='tf')\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X['input_ids'].numpy(), labels, test_size=0.25, random_state=42)\n","\n","# Initialize the BERT model for sequence classification\n","#model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, from_pt=True, use_safetensors=False)\n","model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","\n","# Compile the model\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n"," metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train, np.array(y_train), epochs=3, batch_size=8, validation_data=(X_test, np.array(y_test)))\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X_test, np.array(y_test))\n","display(f\"Accuracy: {accuracy}\")\n","\n","# Predict the sentiment of new text\n","new_text = [\"The product is excellent and I love it.\"]\n","new_text_enc = tokenizer(new_text, padding=True, truncation=True, max_length=10, return_tensors='tf')\n","prediction = model.predict(new_text_enc['input_ids'])\n","display(\"Prediction:\", \"Positive\" if np.argmax(prediction.logits) == 1 else \"Negative\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":512},"id":"L910HTppiv3c","executionInfo":{"status":"error","timestamp":1757389874992,"user_tz":300,"elapsed":100348,"user":{"displayName":"Jimmy Ardiansyah","userId":"06913112371375508078"}},"outputId":"8719eeeb-f0fc-497a-cf6a-d35274c3eea3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n","TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"]},{"output_type":"error","ename":"TypeError","evalue":"'builtins.safe_open' object is not iterable","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3826304566.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Initialize the BERT model for sequence classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, from_pt=True, use_safetensors=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Compile the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2962\u001b[0m                 \u001b[0;31m# We load in TF format here because PT weights often need to be transposed, and this is much\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m                 \u001b[0;31m# faster on GPU. Loading as numpy and transposing on CPU adds several seconds to load times.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2964\u001b[0;31m                 return load_pytorch_state_dict_in_tf2_model(\n\u001b[0m\u001b[1;32m   2965\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2966\u001b[0m                     \u001b[0msafetensors_archive\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_tf_pytorch_utils.py\u001b[0m in \u001b[0;36mload_pytorch_state_dict_in_tf2_model\u001b[0;34m(tf_model, pt_state_dict, tf_inputs, allow_missing_keys, output_loading_info, _prefix, tf_to_pt_weight_rename, ignore_mismatched_sizes, skip_logger_warnings)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;31m# Convert old format to new format if needed from a PyTorch state_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0mtf_keys_to_pt_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpt_state_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0mnew_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"gamma\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'builtins.safe_open' object is not iterable"]}]},{"cell_type":"markdown","source":["**Explain the code snippet above in detail. **\n","\n","___\n","\n","**Type Your ResponseBelow:**  "],"metadata":{"id":"-5_WMIxui3-5"}}]}